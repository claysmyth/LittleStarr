{
    "sourceFile": "model_development/powerband_identification_for_LDA.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 10,
            "patches": [
                {
                    "date": 1681851513005,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1681851649335,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -533,8 +533,9 @@\n     parameters (dict): The parameters dictionary. It must contain the following keys:\n         'TD_Columns' (list): The TD channels to use for the pipeline.\n         'fft_subset_inds' (list): The start and end indices of the fft subset that is used for feature selection. In other words, pre-limit the FFT bins desired to be used for feature selection. \n         'update_rate' (int): The update rate, of which to average the powerbands over.\n+        '\n         'correlation_threshold' (float): The correlation threshold for removing FFT bins with low correlation (below the threshold).\n         'num_features_to_select' (int): The number of features to select in the SequentialForwardFeature selection.\n         'max_clusters' (int): The maximum number of clusters to use for the k-means clustering.\n     sleep_stage_mapping (dict): The sleep stage mapping dictionary. This is used to binarize the sleep stages into 0 and 1.\n"
                },
                {
                    "date": 1681851655011,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -533,9 +533,9 @@\n     parameters (dict): The parameters dictionary. It must contain the following keys:\n         'TD_Columns' (list): The TD channels to use for the pipeline.\n         'fft_subset_inds' (list): The start and end indices of the fft subset that is used for feature selection. In other words, pre-limit the FFT bins desired to be used for feature selection. \n         'update_rate' (int): The update rate, of which to average the powerbands over.\n-        '\n+        'model' (sklearn model): The model to use for the cross validation.\n         'correlation_threshold' (float): The correlation threshold for removing FFT bins with low correlation (below the threshold).\n         'num_features_to_select' (int): The number of features to select in the SequentialForwardFeature selection.\n         'max_clusters' (int): The maximum number of clusters to use for the k-means clustering.\n     sleep_stage_mapping (dict): The sleep stage mapping dictionary. This is used to binarize the sleep stages into 0 and 1.\n"
                },
                {
                    "date": 1681851660197,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -533,9 +533,9 @@\n     parameters (dict): The parameters dictionary. It must contain the following keys:\n         'TD_Columns' (list): The TD channels to use for the pipeline.\n         'fft_subset_inds' (list): The start and end indices of the fft subset that is used for feature selection. In other words, pre-limit the FFT bins desired to be used for feature selection. \n         'update_rate' (int): The update rate, of which to average the powerbands over.\n-        'model' (sklearn model): The model to use for the cross validation.\n+        'model' (sklearn model): The model to use for the cross validation. Can use custom models from \n         'correlation_threshold' (float): The correlation threshold for removing FFT bins with low correlation (below the threshold).\n         'num_features_to_select' (int): The number of features to select in the SequentialForwardFeature selection.\n         'max_clusters' (int): The maximum number of clusters to use for the k-means clustering.\n     sleep_stage_mapping (dict): The sleep stage mapping dictionary. This is used to binarize the sleep stages into 0 and 1.\n"
                },
                {
                    "date": 1681851667546,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -533,9 +533,9 @@\n     parameters (dict): The parameters dictionary. It must contain the following keys:\n         'TD_Columns' (list): The TD channels to use for the pipeline.\n         'fft_subset_inds' (list): The start and end indices of the fft subset that is used for feature selection. In other words, pre-limit the FFT bins desired to be used for feature selection. \n         'update_rate' (int): The update rate, of which to average the powerbands over.\n-        'model' (sklearn model): The model to use for the cross validation. Can use custom models from \n+        'model' (sklearn model): The model to use for the cross validation. Can use custom models from 'embedded_model\n         'correlation_threshold' (float): The correlation threshold for removing FFT bins with low correlation (below the threshold).\n         'num_features_to_select' (int): The number of features to select in the SequentialForwardFeature selection.\n         'max_clusters' (int): The maximum number of clusters to use for the k-means clustering.\n     sleep_stage_mapping (dict): The sleep stage mapping dictionary. This is used to binarize the sleep stages into 0 and 1.\n"
                },
                {
                    "date": 1681851687554,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -533,9 +533,9 @@\n     parameters (dict): The parameters dictionary. It must contain the following keys:\n         'TD_Columns' (list): The TD channels to use for the pipeline.\n         'fft_subset_inds' (list): The start and end indices of the fft subset that is used for feature selection. In other words, pre-limit the FFT bins desired to be used for feature selection. \n         'update_rate' (int): The update rate, of which to average the powerbands over.\n-        'model' (sklearn model): The model to use for the cross validation. Can use custom models from 'embedded_model\n+        'model' (sklearn model): The model to use for the cross validation. Can use custom models from 'embedded_model_classes.py'. If 'None', then use \n         'correlation_threshold' (float): The correlation threshold for removing FFT bins with low correlation (below the threshold).\n         'num_features_to_select' (int): The number of features to select in the SequentialForwardFeature selection.\n         'max_clusters' (int): The maximum number of clusters to use for the k-means clustering.\n     sleep_stage_mapping (dict): The sleep stage mapping dictionary. This is used to binarize the sleep stages into 0 and 1.\n"
                },
                {
                    "date": 1681851693550,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -533,9 +533,9 @@\n     parameters (dict): The parameters dictionary. It must contain the following keys:\n         'TD_Columns' (list): The TD channels to use for the pipeline.\n         'fft_subset_inds' (list): The start and end indices of the fft subset that is used for feature selection. In other words, pre-limit the FFT bins desired to be used for feature selection. \n         'update_rate' (int): The update rate, of which to average the powerbands over.\n-        'model' (sklearn model): The model to use for the cross validation. Can use custom models from 'embedded_model_classes.py'. If 'None', then use \n+        'model' (sklearn model): The model to use for the cross validation. Can use custom models from 'embedded_model_classes.py'. If 'None', then use classic LDA.\n         'correlation_threshold' (float): The correlation threshold for removing FFT bins with low correlation (below the threshold).\n         'num_features_to_select' (int): The number of features to select in the SequentialForwardFeature selection.\n         'max_clusters' (int): The maximum number of clusters to use for the k-means clustering.\n     sleep_stage_mapping (dict): The sleep stage mapping dictionary. This is used to binarize the sleep stages into 0 and 1.\n"
                },
                {
                    "date": 1681851699341,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -594,8 +594,9 @@\n     # corr_chart = alt.Chart(corr_df).mark_line().encode(x='Hz', y='corr', color='key').interactive()\n \n     first_feature_group, X, y = get_train_data_by_corr_threshold(df_fft_subset, fft_corrs, parameters['corr_threshold'], label_col='SleepStageBinary')\n     del(df_fft_subset)\n+    if \n     model = LinearDiscriminantAnalysis(solver='svd')\n     print('Running Sequential Feature Selector...')\n     sfs = SequentialFeatureSelector(model, n_features_to_select=parameters['num_features_to_select'], direction='forward', n_jobs=6, cv=5, scoring=parameters['sfs_scoring'])\n     sfs.fit(X, y)\n"
                },
                {
                    "date": 1681851710191,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -594,9 +594,9 @@\n     # corr_chart = alt.Chart(corr_df).mark_line().encode(x='Hz', y='corr', color='key').interactive()\n \n     first_feature_group, X, y = get_train_data_by_corr_threshold(df_fft_subset, fft_corrs, parameters['corr_threshold'], label_col='SleepStageBinary')\n     del(df_fft_subset)\n-    if \n+    if model is None:\n     model = LinearDiscriminantAnalysis(solver='svd')\n     print('Running Sequential Feature Selector...')\n     sfs = SequentialFeatureSelector(model, n_features_to_select=parameters['num_features_to_select'], direction='forward', n_jobs=6, cv=5, scoring=parameters['sfs_scoring'])\n     sfs.fit(X, y)\n"
                },
                {
                    "date": 1681851715924,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -595,9 +595,11 @@\n \n     first_feature_group, X, y = get_train_data_by_corr_threshold(df_fft_subset, fft_corrs, parameters['corr_threshold'], label_col='SleepStageBinary')\n     del(df_fft_subset)\n     if model is None:\n-    model = LinearDiscriminantAnalysis(solver='svd')\n+        model = LinearDiscriminantAnalysis(solver='svd')\n+    else:\n+        mo\n     print('Running Sequential Feature Selector...')\n     sfs = SequentialFeatureSelector(model, n_features_to_select=parameters['num_features_to_select'], direction='forward', n_jobs=6, cv=5, scoring=parameters['sfs_scoring'])\n     sfs.fit(X, y)\n \n"
                },
                {
                    "date": 1681851722483,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -597,9 +597,9 @@\n     del(df_fft_subset)\n     if model is None:\n         model = LinearDiscriminantAnalysis(solver='svd')\n     else:\n-        mo\n+        model = parameters['model']\n     print('Running Sequential Feature Selector...')\n     sfs = SequentialFeatureSelector(model, n_features_to_select=parameters['num_features_to_select'], direction='forward', n_jobs=6, cv=5, scoring=parameters['sfs_scoring'])\n     sfs.fit(X, y)\n \n"
                }
            ],
            "date": 1681851513005,
            "name": "Commit-0",
            "content": "import duckdb\nimport polars as pl\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport altair as alt\nfrom rcssim import rcs_sim as rcs\nfrom itertools import combinations\nimport sys\nsys.path.append('../databasing')\nfrom database_calls import get_device_as_pl_df, get_gains_from_settings_dict, get_settings_for_pb_calcs\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.cluster import KMeans\n\nDATABASE_PATH = '/media/shortterm_ssd/Clay/databases/duckdb/rcs-db.duckdb'\nALL_TD_COLUMNS = ['TD_BG', 'TD_key2', 'TD_key3']\n\n# TODO: Use function from dataprocessing/data_transforms instead of this one\ndef chunk_df_by_timesegment(df, interval='1s', period='2s', sample_rate=500, align_with_PB_outputs=False, td_columns=['TD_BG', 'TD_key2', 'TD_key3']):\n    \"\"\"\n    Chunk a dataframe into smaller dataframes based on a time interval and period.\n    The period is the length of the time segment, the interval is the time between the start of each time segment.\n    \n    Parameters:\n    df (DataFrame): The dataframe to be chunked\n    interval (str): The time interval between the start of each time segment. Default is '1s'\n    period (str): The length of each time segment. Default is '2s'\n    align_with_PB_outputs (bool): If True, the time segments will be aligned with the Power Band outputs. Default is False.\n    \"\"\"\n    td_cols = [col for col in df.columns if col in td_columns]\n    # TODO: Remove hardcoding of 'SleepStage', should refer to it as a variable\n    if align_with_PB_outputs:\n        df_pb_count = df.join(\n            df.filter(pl.col('Power_Band8').is_not_null()).select(\n                'DerivedTime').with_row_count(),\n            on='DerivedTime', how='left').with_columns(pl.col('row_nr').fill_null(strategy='backward')).rename({'row_nr': 'PB_count'})\n\n        df_pb_count = df_pb_count.with_columns([\n            pl.when( (pl.col('PB_count') % 2) == 0).then(pl.lit(None)).otherwise(pl.col('PB_count')).fill_null(strategy='backward').alias('PB_count_odd'),\n            pl.when( (pl.col('PB_count') % 2) == 1).then(pl.lit(None)).otherwise(pl.col('PB_count')).fill_null(strategy='backward').alias('PB_count_even')\n        ])\n\n        df_pb_count = df_pb_count.groupby(['SleepStage', 'PB_count_even']).agg(\n            [\n                pl.col('DerivedTime'),\n                pl.col('^Power_Band.*$').drop_nulls().first(),\n                pl.col('^TD_.*$'),\n                pl.col(td_cols[0]).count().alias('TD_count')\n            ]).rename({'PB_count_even': 'PB_ind'}).vstack(\n                df_pb_count.groupby(['SleepStage', 'PB_count_odd']).agg(\n                    [\n                        pl.col('DerivedTime'),\n                        pl.col('^Power_Band.*$').drop_nulls().first(),\n                        pl.col('^TD_.*$'),\n                pl.col(td_cols[0]).count().alias('TD_count')\n                    ]).rename({'PB_count_odd': 'PB_ind'})\n        ).select(pl.all().shrink_dtype()).rechunk()\n\n        df_chunked = df_pb_count\n    else:\n        df_grouped = df.sort('localTime').groupby_dynamic('localTime', every=interval, period=period, by=['SessionIdentity', 'SleepStage']).agg([\n            pl.col('DerivedTime'),\n            pl.col('^Power_Band.*$').drop_nulls().first(),\n            pl.col('^TD_.*$'),\n                pl.col(td_cols[0]).count().alias('TD_count')]).select(pl.all().shrink_dtype())\n\n        df_grouped = df_grouped.with_columns(\n                    pl.col(td_cols[0]).arr.eval(pl.element().is_null().any()).alias('TD_null')\n                ).filter((pl.col('TD_count') == int(period[0]) * sample_rate ) &\n                        (pl.col('TD_null').arr.contains(False))\n                        )\n        df_chunked = df_grouped\n    return df_chunked\n\n\ndef add_simulated_ffts(df_chunked, settings, gains, shift=None, td_columns=['TD_BG', 'TD_key2', 'TD_key3']):    \n    \"\"\"\n    Add simulated FFTs to a dataframe of time segments. This function calls the rcs_sim package to simulate RC+S outputs\n    \n    parameters:\n    df_chunked (pl.DataFrame): The dataframe of time segments, as output by chunk_df_by_timesegment\n    settings (dict): The settings dictionary for the simulation (usually replicating the settings used to generate the original FFTs from an RCS session)\n    gains (list): The amp gains to use for the simulation\n    fft_subset_inds (list): The indices of the FFTs to use for the simulation. Default is [2,120]\n\n    returns:\n    df_chunked (pl.DataFrame): The dataframe of time segments with simulated FFTs added as additional columns. Each time domain channel has its own set of simulated FFTs.\n    \"\"\"\n\n    # Select the time domain channels and timestamps to use for the simulating FFTs\n    td_np = df_chunked.select([\n        pl.col('DerivedTime'),\n        pl.col('^TD_.*$')\n    ]).collect().to_numpy()\n\n    td_cols = [col for col in df_chunked.columns if col in td_columns]\n\n    print(td_cols)\n\n    fft_arr = np.zeros((len(td_cols), td_np.shape[0], settings['fft_numBins'][0]))\n\n    sim_settings = settings.copy()\n    if shift:\n        sim_settings['fft_bandFormationConfig'] = [shift]\n\n\n    # Could try to use numba to speed up the loop, or use 'pl.apply' on the dataframe\n    # Simulate FFTs for each time segment, one FFT period at a time\n    def fft_sim_row(i):\n        for j in range(len(td_cols)):\n            fft_arr[j,i], _ = rcssim_wrapper(td_np[i,j+1], td_np[i,0], sim_settings, gains[j])\n\n\n    #@njit(parallel=True)\n    for i in range(td_np.shape[0]):\n        fft_sim_row(i)\n\n    # Join the simulated FFTs to the dataframe\n    df_chunked = df_chunked.with_row_count().join(\n                            pl.LazyFrame({f'fft_{i}': fft_arr[i] for i in range(len(td_cols))}).with_row_count(), \n                            on='row_nr', how='left')\n\n    return df_chunked\ndef expand_fft_arrays_with_subset_deprecated(df, fft_ind_start, vec_length):\n    \"\"\"\n    Take a subset of each Time Domain Channels FFT simulations, and expand each fft bin into it's own column\n\n    parameters:\n    df (pl.DataFrame): The dataframe of time segments with simulated FFTs added as additional columns. Each time domain channel has its own set of simulated FFTs.\n    fft_ind_start (int): The index of the first FFT bin to use\n    vec_length (int): The number of FFT bins to use when calculating the fft subset\n\n    returns:\n    df (pl.DataFrame): The dataframe of time segments with simulated FFTs bins added as individual columns\n    \"\"\"\n    return (df.with_columns([\n                        pl.col('^fft_.*$').arr.slice(fft_ind_start, vec_length)\n                    ])\n                    .with_columns(\n                        pl.col('fft_BG').arr.concat([pl.col('fft_key2'), pl.col('fft_key3')]).arr.to_struct().alias('fft_vec')\n                    )\n                    .unnest('fft_vec')\n                    )\n\n\ndef expand_fft_arrays_with_subset(df, fft_ind_start, vec_length):\n    \"\"\"\n    Take a subset of each Time Domain Channels FFT simulations, and expand each fft bin into it's own column\n\n    parameters:\n    df (pl.DataFrame): The dataframe of time segments with simulated FFTs added as additional columns. Each time domain channel has its own set of simulated FFTs.\n    fft_ind_start (int): The index of the first FFT bin to use\n    vec_length (int): The number of FFT bins to use when calculating the fft subset\n\n    returns:\n    df (pl.DataFrame): The dataframe of time segments with simulated FFTs bins added as individual columns\n    \"\"\"\n    fft_cols = [col for col in df.columns if 'fft' in col]\n    return (df.with_columns([\n                        pl.col('^fft_.*$').arr.slice(fft_ind_start, vec_length)\n                    ])\n                    .with_columns(\n                        pl.col(fft_cols[0]).arr.concat(pl.col(fft_cols[1:])).arr.to_struct().alias('fft_vec')\n                    )\n                    .unnest('fft_vec')\n                    )\n\n\ndef get_fft_corrs(df_chunked, fft_subset_inds=[2,120], label_col='SleepStage', td_columns=['TD_BG', 'TD_key2', 'TD_key3']) -> np.ndarray:\n    \"\"\"\n    Calculate the correlation between each FFT bin and the label columns (SleepStage, or other)\n\n    parameters:\n    df_chunked (pl.DataFrame): The dataframe of time segments with simulated FFTs added as additional columns. Each time domain channel has its own set of simulated FFTs.\n    fft_subset_inds (list): The indices of the subset of each FFT output to use for the simulation. Default is [2,120]\n    label_col (str): The column to use for the label, for which each frequency bin will be compared to in a pearson correlation measure. Default is 'SleepStage'\n\n    returns:\n    df_fft_vec (pl.DataFrame): The dataframe with the desired subset of simulated FFTs bins for each FFT interval added as individual columns\n    fft_corrs (np.ndarray): The correlation between each FFT bin and the label column\n    \"\"\"\n\n    fft_ind_start = fft_subset_inds[0]\n    vec_length = fft_subset_inds[1] - fft_subset_inds[0]\n\n    df_fft_vec = expand_fft_arrays_with_subset(df_chunked, fft_ind_start=fft_ind_start, vec_length=vec_length)\n\n    # Could convert fft_corrs to be a pl.dataframe with discord trick by @ms. This would allow it to be lazy and save on memory.\n    fft_corrs = df_fft_vec.select([\n                        pl.corr(f\"field_{i}\", pl.col(label_col).cast(pl.Float64), method='pearson').alias(f\"field_{i}_corr\") for i in range(vec_length*len(td_columns))\n                    ]).to_numpy().squeeze()\n    \n    return df_fft_vec, fft_corrs\n\n\ndef get_train_data_by_corr_threshold(df, fft_corrs, corr_threshold, label_col='SleepStageBinary'):\n    \"\"\"\n    Reduce features for powerband selection, by selecting the FFT bins that have a correlation above the threshold\n\n    parameters:\n    df (pl.DataFrame): The dataframe of time segments (i.e. individual fft windows are single rows) with simulated FFTs added as additional columns.\n    corr_threshold (float): The threshold for the correlation between the FFT bins and the label column. Only FFT bins with a correlation above this threshold will kept for future feature selection.\n    label_col (str): The column to use for the label, for which each frequency bin will be compared to in a pearson correlation measure. Default is 'SleepStageBinary'\n    \"\"\"\n    X = df.select(pl.col(\"^field_.*$\")).to_numpy()\n    feature_group = np.argwhere(np.abs(fft_corrs) > corr_threshold)\n    X = X[:, np.where(np.abs(fft_corrs) > corr_threshold)[0]]\n    y = df.select(pl.col(label_col)).to_numpy().squeeze()\n    return feature_group, X, y\n\n\ndef get_PB_combinations_deprecated(sfs, feature_group, max_clusters=8):\n    PB_groupings = []\n    for i in range(2, max_clusters+1): \n        kmeans = KMeans(n_clusters=i, random_state=0).fit(sfs.get_support(indices=True)[:, np.newaxis])\n\n        pbs = [[feature_group[np.argwhere(kmeans.labels_ == j).squeeze()].min(), \n                feature_group[np.argwhere(kmeans.labels_ == j).squeeze()].max()] for j in range(i)]\n        \n        cluster_df = pd.DataFrame({'x':sfs.get_support(indices=True), 'y':np.zeros(sfs.get_support(indices=True).shape[0]), 'color':kmeans.labels_})\n        if i == 2:\n            chart = alt.Chart(cluster_df).mark_point().encode(x='x', y='y', color=alt.Color('color', scale=alt.Scale(scheme='category20b')))\n        else:\n            chart &= alt.Chart(cluster_df).mark_point().encode(x='x', y='y', color=alt.Color('color', scale=alt.Scale(scheme='category20b')))\n        \n        PB_groupings.append(pbs)\n    \n    pb_combos = []\n    for pbs in PB_groupings:\n        if len(pbs) <= 4:\n            pb_combos.append(pbs)\n        else:\n            pb_combos.append(list(combinations(pbs, 4)))\n            \n    return pb_combos, chart\n\n\ndef get_PB_combinations(features, max_clusters=8):\n    \"\"\"\n    Takes in the features and returns the possible powerband combinations. Features are fft indices as determined by SequentialFeatureSelector.\n\n    parameters:\n    features (np.ndarray): The fft indices that were selected by SequentialFeatureSelector. They chould correspond to the indicies within the entire fft vector (i.e. the three channels fft outputs horizontally concatenated), \n        not the subset of the FFTs that was used for feature selection.\n    max_clusters (int): The maximum number of possible powerbands to consider for selection. Default is 8.\n\n    returns:\n    pb_combos (list): A list of lists of lists. The first list is the combination of powerbands for the corresponding cluster amount, the second list is the individual powerbands, \n    and the third list is the start and end indices of the powerband.\n    chart (altair.Chart): An interactive chart of the powerband combinations. The x-axis is the fft indices, the y-axis is irrelevant, and the color is the cluster number.\n    \"\"\"\n    PB_groupings = []\n    for i in range(2, max_clusters+1): \n        kmeans = KMeans(n_clusters=i, random_state=0).fit(features[:, np.newaxis])\n\n        # Extract first and last index of each cluster, to be used as powerband boundaries\n        pbs = [[features[np.argwhere(kmeans.labels_ == j).squeeze()].min(), \n                features[np.argwhere(kmeans.labels_ == j).squeeze()].max()] for j in range(i)]\n        \n        cluster_df = pd.DataFrame({'x':features, 'y':np.zeros(features.shape[0]), 'color':kmeans.labels_})\n        base = alt.Chart(cluster_df).mark_point().encode(x='x', y='y', color=alt.Color('color', scale=alt.Scale(scheme='category20b'))).interactive()\n        if i == 2:\n            chart = base\n        else:\n            chart &= base\n        \n        PB_groupings.append(pbs)\n    \n    pb_combos = []\n    for pbs in PB_groupings:\n        if len(pbs) <= 4:\n            pb_combos.append(pbs)\n        else:\n            # Get all combinations of 4 powerbands chosen from the possible powerbands in that cluster (e.g. if num clusters is 8, add all combinations of 4 powerbands from the 8 powerbands)\n            pb_combos.append(list(combinations(pbs, 4)))\n            \n    return pb_combos, chart\n\n\ndef recover_original_feature_inds(feature_list, fft_subset_inds, fft_length):\n    \"\"\"\n    Recover the original fft indices (i.e. the indices of the fft bins from the FFT vector that corresponds to the concatenated simulations of each TD channel) from the feature list. This typically refers to putative powerband edges. \n    Recall the features were collected from the fft subset indices.\n\n    parameters:\n    feature_list (list): The list of features.\n    fft_subset_inds (list): The start and end indices of the fft subset that was used for feature selection.\n    fft_length (int): The length of the simulated fft vector for each TD channel (e.g. 512 for an FFT window of 1024 on a 500 TD sampling rate).\n\n    returns:\n    list: The list of original fft indices.\n    \"\"\"\n    vec_length = fft_subset_inds[1] - fft_subset_inds[0]\n    vec_start = fft_subset_inds[0]\n\n    return [(\n        (i - (vec_length * (i // vec_length)) ) # remove channel offset of index\n        + vec_start # add the start index of the fft vector subset\n        + (i // vec_length) * fft_length) # recover channel offset of original fft vector\n        for i in feature_list]\n\n\ndef get_df_from_pb_combos(pb_combos):\n    \"\"\"\n    Takes in the powerband combinations and returns a polars dataframe with the powerband combinations as columns.\n    parameters:\n    pb_combos (list): A list of lists of lists. The first list is the combination of powerbands for the corresponding cluster amount, the second list is the individual powerbands,\n    and the third list is the start and end indices of the powerband.\n\n    returns:\n    pl.DataFrame: A polars dataframe with the powerband combinations as columns.\n    \"\"\"\n    tmp = [[list(sub_ele) for sub_ele in ele] for ele in pb_combos]\n    tmp2 = []\n    for ele in tmp:\n        if len(ele) <= 4:\n            _ = [ele.append([None,None]) for i in range(4-len(ele))]\n            tmp2.append(ele)\n        else:\n            [tmp2.append(sub_ele) for sub_ele in ele]\n    return pl.DataFrame(tmp2, schema=['PB1', 'PB2', 'PB3', 'PB4'])\n\n\ndef get_training_data_for_LDA(df_fft, pbs, update_rate, label_col='SleepStageBinary') -> Tuple(np.ndarray, np.ndarray):\n   \"\"\"\n    Takes in the fft dataframe and the powerband combinations and returns a numpy array of the calculated powerbands, averaged over the update rate, and a numpy array of the labels.\n    parameters:\n    df_fft (pl.DataFrame): The fft dataframe.\n    pbs (polards.DataFrame): The powerband combinations dataframe.\n    update_rate (int): The update rate, of which to average the powerbands over.\n    label_col (str): The label column name. Default is 'SleepStageBinary'.\n\n    returns:\n    X (np.ndarray): The calculated powerbands, averaged over the update rate.\n    y (np.ndarray): The labels.\n   \"\"\"\n   simulated_ffts = (\n    df_fft\n    .select(\n            [\n                pl.col('fft_vec').arr.slice(pbs[i][0], (pbs[i][1] - pbs[i][0] + 1)).arr.sum().alias(f'Power_Band{i+1}') \n                for i in range(len(pbs)) if pbs[i][0] is not None\n            ] + \n            [pl.col(label_col)]\n    )\n    .with_row_count()\n    .with_columns([\n        pl.col('row_nr') // update_rate\n    ])\n    .groupby(['row_nr'])\n    .agg(\n        [\n            pl.col(f'Power_Band{i+1}').mean() \n            for i in range(len(pbs)) if pbs[i][0] is not None\n        ] + \n        [pl.col(label_col).last().alias(label_col)]\n      )\n   )\n   \n   X = simulated_ffts.select(pl.col(\"^Power_Band.*$\")).to_numpy()\n   y = simulated_ffts.select(pl.col(label_col)).to_numpy().squeeze()\n   return X, y\n    \n# The below function is intended to identify potential powerband combinations for maximizing sleep stage classification via LDA (or alternative models). Executes the above functions in order. \ndef hyperparameter_search_pipeline(device, parameters, sleep_stage_mapping, out_file_path, np_random_seed=0):\n    \"\"\"\n    Executes the hyperparameter search pipeline for a given device. The pipeline is as follows:\n    1. Get the device's fft dataframe.\n    2. Correlate FFT bins with the binarized sleep stage labels.\n    3. Remove FFT bins with low correlation (below the threshold)\n    4. Use SequentialForwardFeature selection to get the n best remaining FFT bins for sleep stage classification.\n    5. Cluster the n best FFT bins into 2 to k clusters.\n    5. Get the powerband combinations for each cluster amount.\n    6. Use the powerbands combinations and update rate to procure training data for LDAs.\n    7. Run an LDA cross validation on each powerband combination.\n    8. Get the cross-validated LDA model's scores for each powerband combination.\n    9. Save the powerband combinations and corresponding scores to a parquet file.\n\n    parameters:\n    device (str): The device to execute the pipeline on.\n    parameters (dict): The parameters dictionary. It must contain the following keys:\n        'TD_Columns' (list): The TD channels to use for the pipeline.\n        'fft_subset_inds' (list): The start and end indices of the fft subset that is used for feature selection. In other words, pre-limit the FFT bins desired to be used for feature selection. \n        'update_rate' (int): The update rate, of which to average the powerbands over.\n        'correlation_threshold' (float): The correlation threshold for removing FFT bins with low correlation (below the threshold).\n        'num_features_to_select' (int): The number of features to select in the SequentialForwardFeature selection.\n        'max_clusters' (int): The maximum number of clusters to use for the k-means clustering.\n    sleep_stage_mapping (dict): The sleep stage mapping dictionary. This is used to binarize the sleep stages into 0 and 1.\n    out_file_path (str): The path to save the parquet file to.\n    \"\"\"\n    assert len(parameters['TD_Columns']) > 1, 'Must select at least 2 TD channels to run hyperparameter search pipeline.'\n\n    print('Executing Device: ', device)\n    np.random.seed(np_random_seed)\n    con = duckdb.connect(database=DATABASE_PATH, read_only=True)\n    \n    df = get_device_as_pl_df(device, con, lazy=True)\n\n    # Keep only desired TD columns\n    df = df.select(pl.all().exclude( list(set(ALL_TD_COLUMNS) - set(parameters['TD_Columns'])) ) )\n    print('Analyzing: ' + ', '.join(df.select(pl.col('^TD.*$')).columns))\n\n    sessions = df.select('SessionIdentity').unique().collect().to_dict(\n        as_series=False)['SessionIdentity']\n    settings = get_settings_for_pb_calcs(device, con, sessions, 'SessionIdentity')\n    gains = get_gains_from_settings_dict(\n        settings, sessions, 'SessionIdentity', device, con)\n    \n    gains_tmp = []\n    for channel in parameters['TD_Columns']:\n        if channel == 'TD_BG':\n            # TODO: TD_BG need not be gains[0]. Could be gains[1]... I should use the TDSettings table to get correct gain for subcortical channel\n            gains_tmp.append(gains[0])\n        if channel == 'TD_key2':\n            gains_tmp.append(gains[1])\n        if channel == 'TD_key3':\n            gains_tmp.append(gains[2])\n    gains = gains_tmp\n    print(gains)\n\n    print(settings)\n    print('ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0')\n    fft_length = settings['fft_numBins'][0]\n\n    df_chunked = chunk_df_by_timesegment(df)\n    print(df_chunked.columns) # here\n    print(f'Simulating FFTs for TD chunks of size {settings[\"fft_size\"][0] - settings[\"fft_size\"][0]%250} samples')\n    df_chunked = add_simulated_ffts(df_chunked, settings, gains)\n    df_chunked = df_chunked.with_columns(pl.col('SleepStage').map_dict(sleep_stage_mapping).alias('SleepStageBinary'))\n\n    NUM_TD_CHANNELS = len(parameters['TD_Columns'])\n\n    df_chunked = df_chunked.collect()\n\n    df_fft_subset, fft_corrs = get_fft_corrs(df_chunked, parameters['fft_subset_inds'], label_col='SleepStageBinary', td_columns=parameters['TD_Columns'])\n\n    \n    fft_subset_length = parameters['fft_subset_inds'][1] - parameters['fft_subset_inds'][0]\n    # corr_df = pd.DataFrame({'Hz': 0.48*(np.arange(fft_subset_length) + parameters['fft_subset_inds'][0]), 'BG': fft_corrs[:fft_subset_length], 'key2': fft_corrs[fft_subset_length:fft_subset_length*2], \n    #                         'key3': fft_corrs[fft_subset_length*2:]}).melt(id_vars='Hz', value_vars=['BG', 'key2', 'key3'], var_name='key', value_name='corr')\n    # corr_chart = alt.Chart(corr_df).mark_line().encode(x='Hz', y='corr', color='key').interactive()\n\n    first_feature_group, X, y = get_train_data_by_corr_threshold(df_fft_subset, fft_corrs, parameters['corr_threshold'], label_col='SleepStageBinary')\n    del(df_fft_subset)\n    model = LinearDiscriminantAnalysis(solver='svd')\n    print('Running Sequential Feature Selector...')\n    sfs = SequentialFeatureSelector(model, n_features_to_select=parameters['num_features_to_select'], direction='forward', n_jobs=6, cv=5, scoring=parameters['sfs_scoring'])\n    sfs.fit(X, y)\n\n\n    second_feature_group = first_feature_group[sfs.get_support(indices=True)]\n    second_feature_group_original_inds = np.array(recover_original_feature_inds(second_feature_group, parameters['fft_subset_inds'], fft_length)).squeeze()\n    pb_combos, pb_chart = get_PB_combinations(second_feature_group_original_inds, max_clusters=parameters['max_clusters'])\n    df_pbs_corrected = get_df_from_pb_combos(pb_combos)\n\n    df_pbs_corrected = df_pbs_corrected.join(pl.DataFrame({'UpdateRate': [2, 5, 10, 15, 30]}), how='cross')\n\n    fft_cols = [col for col in df_chunked.columns if 'fft' in col]\n\n    df_chunked = df_chunked.select([\n        pl.col('SleepStage'),\n        pl.col('SleepStageBinary'),\n        pl.col(fft_cols[0]).arr.concat(pl.col(fft_cols[1:])).alias('fft_vec')\n    ])\n    try:\n        assert fft_length == df_chunked.select(\n                pl.col('fft_vec').arr.lengths()\n            ).unique().item() / NUM_TD_CHANNELS\n    except AssertionError:\n        print('fft_length is not equal to the length of the fft_vec column.')\n        print('fft_length: ', fft_length)\n        print('length of fft_vec column: ', df_chunked.select(\n                pl.col('fft_vec').arr.lengths()\n            ).unique().item() / NUM_TD_CHANNELS)\n\n\n    print('Searching over powerband combos...')\n    scores = {'test_accuracy': [], 'test_roc_auc': [], 'test_balanced_accuracy': [], 'test_recall': [], 'test_precision': [], 'test_tnr': []}\n    scores_stds = {'test_accuracy': [], 'test_roc_auc': [], 'test_balanced_accuracy': [], 'test_recall': [], 'test_precision': [], 'test_tnr': []}\n    for i in range(df_pbs_corrected.height):\n        pbs = [value for value in df_pbs_corrected.select(pl.exclude('UpdateRate'))[i].to_dicts()[0].values()]\n        X, y = get_training_data_for_LDA(df_chunked, pbs, df_pbs_corrected[i,'UpdateRate'], label_col='SleepStageBinary')\n\n        # NOTE: 'roc_auc' gets label predictions with clf.predict_proba(X)[:, 1], which allows more thresholds to be tested. \n        # clf.predict_proba(X)[:, 1] is the probability of the positive class (1). clf.predict_proba(X)[:, 0] is the probability of the negative class (0)\n        score_dict = {'accuracy': 'accuracy', 'roc_auc': 'roc_auc', 'balanced_accuracy': 'balanced_accuracy', 'recall': 'recall', 'precision': 'precision'}\n\n        cv_results = cross_validate(model, X, y, cv=5,\n                        scoring=score_dict, n_jobs=5)\n        \n        [scores[k].extend([np.mean(v)]) for (k, v) in cv_results.items() if k in scores.keys()]\n        [scores_stds[k].extend([np.std(v)]) for (k, v) in cv_results.items() if k in scores_stds.keys()]\n\n        tnr = np.array(cv_results['test_balanced_accuracy']) * 2 - cv_results['test_recall']\n        scores['test_tnr'].extend([np.mean(tnr)])\n        scores_stds['test_tnr'].extend([np.std(tnr)])\n        \n\n\n    df_hyperparams = pl.concat([df_pbs_corrected, pl.DataFrame(scores).rename(\n        {'test_accuracy': 'Acc', 'test_roc_auc': 'AUC', 'test_balanced_accuracy': 'BalAcc', 'test_recall': 'TPR', 'test_precision': 'Precision', 'test_tnr': 'TNR'}),\n        pl.DataFrame(scores_stds).rename({'test_accuracy': 'Acc_std', 'test_roc_auc': 'AUC_std', 'test_balanced_accuracy': 'BalAcc_std', 'test_recall': 'TPR_std', 'test_precision': 'precision_std', 'test_tnr': 'TNR_std'})\n        ], how='horizontal')\n\n    df_hyperparams.write_parquet(out_file_path)\n    # corr_chart.save(BASE_PATH + 'sleepstage_corr.png')\n\n    # return (device, df_hyperparams, corr_chart.properties(title=f'{device}'), pb_chart.properties(title=f'{device}'))\n    return (device, df_hyperparams, pb_chart.properties(title=f'{device}'))\n\n\n# The below function is intended to identify potential powerband combinations for maximizing sleep stage classification via LDA (or alternative models). Executes the above functions in order. \ndef hyperparameter_search_pipeline(device, parameters, sleep_stage_mapping, out_file_path, np_random_seed=0):\n    \"\"\"\n    Executes the hyperparameter search pipeline for a given device. The pipeline is as follows:\n    1. Get the device's fft dataframe.\n    2. Correlate FFT bins with the binarized sleep stage labels.\n    3. Remove FFT bins with low correlation (below the threshold)\n    4. Use SequentialForwardFeature selection to get the n best remaining FFT bins for sleep stage classification.\n    5. Cluster the n best FFT bins into 2 to k clusters.\n    5. Get the powerband combinations for each cluster amount.\n    6. Use the powerbands combinations and update rate to procure training data for LDAs.\n    7. Run an LDA cross validation on each powerband combination.\n    8. Get the cross-validated LDA model's scores for each powerband combination.\n    9. Save the powerband combinations and corresponding scores to a parquet file.\n\n    parameters:\n    device (str): The device to execute the pipeline on.\n    parameters (dict): The parameters dictionary. It must contain the following keys:\n        'TD_Columns' (list): The TD channels to use for the pipeline.\n        'fft_subset_inds' (list): The start and end indices of the fft subset that is used for feature selection. In other words, pre-limit the FFT bins desired to be used for feature selection. \n        'update_rate' (int): The update rate, of which to average the powerbands over.\n        'correlation_threshold' (float): The correlation threshold for removing FFT bins with low correlation (below the threshold).\n        'num_features_to_select' (int): The number of features to select in the SequentialForwardFeature selection.\n        'max_clusters' (int): The maximum number of clusters to use for the k-means clustering.\n    sleep_stage_mapping (dict): The sleep stage mapping dictionary. This is used to binarize the sleep stages into 0 and 1.\n    out_file_path (str): The path to save the parquet file to.\n    \"\"\"\n    assert len(parameters['TD_Columns']) > 1, 'Must select at least 2 TD channels to run hyperparameter search pipeline.'\n\n    print('Executing Device: ', device)\n    np.random.seed(np_random_seed)\n    con = duckdb.connect(database=DATABASE_PATH, read_only=True)\n    \n    df = get_device_as_pl_df(device, con, lazy=True)\n\n    # Keep only desired TD columns\n    df = df.select(pl.all().exclude( list(set(ALL_TD_COLUMNS) - set(parameters['TD_Columns'])) ) )\n    print('Analyzing: ' + ', '.join(df.select(pl.col('^TD.*$')).columns))\n\n    sessions = df.select('SessionIdentity').unique().collect().to_dict(\n        as_series=False)['SessionIdentity']\n    settings = get_settings_for_pb_calcs(device, con, sessions, 'SessionIdentity')\n    gains = get_gains_from_settings_dict(\n        settings, sessions, 'SessionIdentity', device, con)\n    \n    gains_tmp = []\n    for channel in parameters['TD_Columns']:\n        if channel == 'TD_BG':\n            # TODO: TD_BG need not be gains[0]. Could be gains[1]... I should use the TDSettings table to get correct gain for subcortical channel\n            gains_tmp.append(gains[0])\n        if channel == 'TD_key2':\n            gains_tmp.append(gains[1])\n        if channel == 'TD_key3':\n            gains_tmp.append(gains[2])\n    gains = gains_tmp\n    print(gains)\n\n    print(settings)\n    print('ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0')\n    fft_length = settings['fft_numBins'][0]\n\n    df_chunked = chunk_df_by_timesegment(df)\n    print(df_chunked.columns) # here\n    print(f'Simulating FFTs for TD chunks of size {settings[\"fft_size\"][0] - settings[\"fft_size\"][0]%250} samples')\n    df_chunked = add_simulated_ffts(df_chunked, settings, gains)\n    df_chunked = df_chunked.with_columns(pl.col('SleepStage').map_dict(sleep_stage_mapping).alias('SleepStageBinary'))\n\n    NUM_TD_CHANNELS = len(parameters['TD_Columns'])\n\n    df_chunked = df_chunked.collect()\n\n    df_fft_subset, fft_corrs = get_fft_corrs(df_chunked, parameters['fft_subset_inds'], label_col='SleepStageBinary', td_columns=parameters['TD_Columns'])\n\n    \n    fft_subset_length = parameters['fft_subset_inds'][1] - parameters['fft_subset_inds'][0]\n    # corr_df = pd.DataFrame({'Hz': 0.48*(np.arange(fft_subset_length) + parameters['fft_subset_inds'][0]), 'BG': fft_corrs[:fft_subset_length], 'key2': fft_corrs[fft_subset_length:fft_subset_length*2], \n    #                         'key3': fft_corrs[fft_subset_length*2:]}).melt(id_vars='Hz', value_vars=['BG', 'key2', 'key3'], var_name='key', value_name='corr')\n    # corr_chart = alt.Chart(corr_df).mark_line().encode(x='Hz', y='corr', color='key').interactive()\n\n    first_feature_group, X, y = get_train_data_by_corr_threshold(df_fft_subset, fft_corrs, parameters['corr_threshold'], label_col='SleepStageBinary')\n    del(df_fft_subset)\n    model = LinearDiscriminantAnalysis(solver='svd')\n    print('Running Sequential Feature Selector...')\n    sfs = SequentialFeatureSelector(model, n_features_to_select=parameters['num_features_to_select'], direction='forward', n_jobs=6, cv=5, scoring=parameters['sfs_scoring'])\n    sfs.fit(X, y)\n\n\n    second_feature_group = first_feature_group[sfs.get_support(indices=True)]\n    second_feature_group_original_inds = np.array(recover_original_feature_inds(second_feature_group, parameters['fft_subset_inds'], fft_length)).squeeze()\n    pb_combos, pb_chart = get_PB_combinations(second_feature_group_original_inds, max_clusters=parameters['max_clusters'])\n    df_pbs_corrected = get_df_from_pb_combos(pb_combos)\n\n    df_pbs_corrected = df_pbs_corrected.join(pl.DataFrame({'UpdateRate': [2, 5, 10, 15, 30]}), how='cross')\n\n    fft_cols = [col for col in df_chunked.columns if 'fft' in col]\n\n    df_chunked = df_chunked.select([\n        pl.col('SleepStage'),\n        pl.col('SleepStageBinary'),\n        pl.col(fft_cols[0]).arr.concat(pl.col(fft_cols[1:])).alias('fft_vec')\n    ])\n    try:\n        assert fft_length == df_chunked.select(\n                pl.col('fft_vec').arr.lengths()\n            ).unique().item() / NUM_TD_CHANNELS\n    except AssertionError:\n        print('fft_length is not equal to the length of the fft_vec column.')\n        print('fft_length: ', fft_length)\n        print('length of fft_vec column: ', df_chunked.select(\n                pl.col('fft_vec').arr.lengths()\n            ).unique().item() / NUM_TD_CHANNELS)\n\n\n    print('Searching over powerband combos...')\n    scores = {'test_accuracy': [], 'test_roc_auc': [], 'test_balanced_accuracy': [], 'test_recall': [], 'test_precision': [], 'test_tnr': []}\n    scores_stds = {'test_accuracy': [], 'test_roc_auc': [], 'test_balanced_accuracy': [], 'test_recall': [], 'test_precision': [], 'test_tnr': []}\n    for i in range(df_pbs_corrected.height):\n        pbs = [value for value in df_pbs_corrected.select(pl.exclude('UpdateRate'))[i].to_dicts()[0].values()]\n        X, y = get_training_data_for_LDA(df_chunked, pbs, df_pbs_corrected[i,'UpdateRate'], label_col='SleepStageBinary')\n\n        # NOTE: 'roc_auc' gets label predictions with clf.predict_proba(X)[:, 1], which allows more thresholds to be tested. \n        # clf.predict_proba(X)[:, 1] is the probability of the positive class (1). clf.predict_proba(X)[:, 0] is the probability of the negative class (0)\n        score_dict = {'accuracy': 'accuracy', 'roc_auc': 'roc_auc', 'balanced_accuracy': 'balanced_accuracy', 'recall': 'recall', 'precision': 'precision'}\n\n        cv_results = cross_validate(model, X, y, cv=5,\n                        scoring=score_dict, n_jobs=5)\n        \n        [scores[k].extend([np.mean(v)]) for (k, v) in cv_results.items() if k in scores.keys()]\n        [scores_stds[k].extend([np.std(v)]) for (k, v) in cv_results.items() if k in scores_stds.keys()]\n\n        tnr = np.array(cv_results['test_balanced_accuracy']) * 2 - cv_results['test_recall']\n        scores['test_tnr'].extend([np.mean(tnr)])\n        scores_stds['test_tnr'].extend([np.std(tnr)])\n        \n\n\n    df_hyperparams = pl.concat([df_pbs_corrected, pl.DataFrame(scores).rename(\n        {'test_accuracy': 'Acc', 'test_roc_auc': 'AUC', 'test_balanced_accuracy': 'BalAcc', 'test_recall': 'TPR', 'test_precision': 'Precision', 'test_tnr': 'TNR'}),\n        pl.DataFrame(scores_stds).rename({'test_accuracy': 'Acc_std', 'test_roc_auc': 'AUC_std', 'test_balanced_accuracy': 'BalAcc_std', 'test_recall': 'TPR_std', 'test_precision': 'precision_std', 'test_tnr': 'TNR_std'})\n        ], how='horizontal')\n\n    df_hyperparams.write_parquet(out_file_path)\n    # corr_chart.save(BASE_PATH + 'sleepstage_corr.png')\n\n    # return (device, df_hyperparams, corr_chart.properties(title=f'{device}'), pb_chart.properties(title=f'{device}'))\n    return (device, df_hyperparams, pb_chart.properties(title=f'{device}'))"
        }
    ]
}