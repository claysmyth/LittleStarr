{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "#from scipy import signal\n",
    "#import mlflow\n",
    "from rcssim import rcs_sim as rcs\n",
    "import plotly.express as px\n",
    "from typing import Tuple\n",
    "import vegafusion as vf\n",
    "#from joypy import joyplot\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import combinations\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, balanced_accuracy_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 13:37:38,846\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "num_GB_to_allocate = 50\n",
    "memory_allocation_in_bytes = num_GB_to_allocate * 1024**3\n",
    "allocate_memory = False\n",
    "base_path = '/media/shortterm_ssd/Clay/ray_spill/'\n",
    "# The object_store_memory flag tells ray how much memory to allocate to the object store (stored in RAM, as a RAM disk. The object store is written to tmpfs, mounted on /dev/shm),\n",
    "# before spill over to disk occurs. One should consider using the allocate_memory flag if RAM is tight and spillover to disk is desired and controlled.\n",
    "if allocate_memory:\n",
    "    ray.init(object_store_memory=memory_allocation_in_bytes,\n",
    "             _system_config={\n",
    "                            \"max_io_workers\": 4,  # More IO workers for parallelism.\n",
    "                                \"object_spilling_config\": json.dumps(\n",
    "                                    {\n",
    "                                        \"type\": \"filesystem\",\n",
    "                                        \"params\": {\n",
    "                                            # Multiple directories can be specified to distribute\n",
    "                                            # IO across multiple mounted physical devices.\n",
    "                                            \"directory_path\": [\n",
    "                                            base_path+\"/tmp/spill\",\n",
    "                                            base_path+\"/tmp/spill_1\",\n",
    "                                            base_path+\"/tmp/spill_2\",\n",
    "                                            ]\n",
    "                                        },\n",
    "                                    }\n",
    "                            )\n",
    "             })\n",
    "else:\n",
    "    # I don't think spill-over is working correctly with current config... Raylet was killing jobs because of 'out of memory' errors.\n",
    "    ray.init(_system_config={\n",
    "                            \"max_io_workers\": 4,  # More IO workers for parallelism.\n",
    "                            \"object_spilling_config\": json.dumps(\n",
    "                                    {\n",
    "                                        \"type\": \"filesystem\",\n",
    "                                        \"params\": {\n",
    "                                            # Multiple directories can be specified to distribute\n",
    "                                            # IO across multiple mounted physical devices.\n",
    "                                            \"directory_path\": [\n",
    "                                            base_path+\"/tmp/spill\",\n",
    "                                            base_path+\"/tmp/spill_1\",\n",
    "                                            base_path+\"/tmp/spill_2\",\n",
    "                                            ]\n",
    "                                        },\n",
    "                                    }\n",
    "                            )\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VegaFusion Widget Enabled\n",
      "Plotly Resampler Enabled\n",
      "VS Code\n"
     ]
    }
   ],
   "source": [
    "import plotly.io as pio\n",
    "alt.data_transformers.disable_max_rows()\n",
    "dataspell = False\n",
    "use_data_server = False # Use data server if you want to use the Vega Editor window\n",
    "use_browser_for_plotly=False\n",
    "remote = False\n",
    "\n",
    "# The below commands allow for Altair to plot large data sets.\n",
    "\n",
    "if dataspell:\n",
    "    if use_data_server:\n",
    "        alt.data_transformers.enable('data_server') # <- Exports to temporary localHost URL\n",
    "        print('Data Server Enabled')\n",
    "    else:\n",
    "        vf.enable()\n",
    "        print('VegaFusion Enabled')\n",
    "    if use_browser_for_plotly:\n",
    "        pio.renderers.default = 'browser'\n",
    "    print(\"DataSpell Setup\")\n",
    "else:\n",
    "    if not remote:\n",
    "        # DO NOT RUN BELOW IN REMOTE VSCODE.\n",
    "        if use_data_server:\n",
    "            alt.data_transformers.enable('data_server') # <- Exports to temporary localHost URL\n",
    "            print('Data Server Enabled')\n",
    "        else:\n",
    "            # Vegafusion allows importing polars dataframes into Altair charts directly, so should use this widget whenever possible.\n",
    "            vf.enable_widget(data_dir='/media/shortterm_ssd/Clay/altairdata/')\n",
    "            print('VegaFusion Widget Enabled')\n",
    "\n",
    "    if use_browser_for_plotly:\n",
    "        pio.renderers.default = 'browser'\n",
    "        print(\"Plotly Browser Enabled\")\n",
    "\n",
    "    from plotly_resampler import register_plotly_resampler, unregister_plotly_resampler\n",
    "    register_plotly_resampler(mode='auto')\n",
    "    print(\"Plotly Resampler Enabled\")\n",
    "    print(\"VS Code\")\n",
    "'''\n",
    "Available templates:\n",
    "        ['ggplot2', 'seaborn', 'simple_white', 'plotly',\n",
    "         'plotly_white', 'plotly_dark', 'presentation', 'xgridoff',\n",
    "         'ygridoff', 'gridon', 'none']\n",
    "'''\n",
    "pio.templates.default = \"plotly_white\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cd60353a0143f5bd3a6f2b7a0b9852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VegaFusionWidget(spec='{\\n  \"config\": {\\n    \"view\": {\\n      \"continuousWidth\": 400,\\n      \"continuousHeight…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if altair displays\n",
    "tmp_df = pd.DataFrame({'A': np.arange(50), 'B': np.arange(50,100)})\n",
    "alt.Chart(tmp_df).mark_point().encode(\n",
    "    x='A:Q',\n",
    "    y='B:Q',\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1b279d4d6e42619106beac98ed89c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidgetResampler({\n",
       "    'data': [{'alignmentgroup': 'True',\n",
       "              'hovertemplate': 'variable=%{x}<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1321736)\u001b[0m Analayzing: TD_BG, TD_key2, TD_key3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 12:22:25,084\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1331028, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/2533931492.py\", line 27, in hyperparameter_search_pipeline\n",
      "IndexError: index 3 is out of bounds for axis 0 with size 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m Analyzing: TD_key2, TD_key3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 12:22:47,090\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1330970, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/2533931492.py\", line 27, in hyperparameter_search_pipeline\n",
      "IndexError: index 3 is out of bounds for axis 0 with size 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m [232 233 231]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1331028)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1331028)\u001b[0m [232 231 234]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 12:44:56,336\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1330970, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/3832721743.py\", line 28, in hyperparameter_search_pipeline\n",
      "IndexError: index 3 is out of bounds for axis 0 with size 3\n",
      "2023-04-04 12:45:05,337\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1331028, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/3832721743.py\", line 28, in hyperparameter_search_pipeline\n",
      "IndexError: index 3 is out of bounds for axis 0 with size 3\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-04 13:18:00,441 E 1330841 1330841] (raylet) node_manager.cc:3040: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 569141cbc5ebfde1dd31aa81dd7722ee96f640efb88b175823a975ce, IP: 10.37.129.11) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.37.129.11`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m [235, 233]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m {'samplingRate': [500], 'gain_1': [233], 'gain_2': [234], 'gain_3': [235], 'gain_4': [233], 'fft_bandFormationConfig': [3], 'fft_interval': [1000], 'fft_size': [1024], 'fft_windowLoad': ['100% Hann'], 'fft_numBins': [512], 'fft_binWidth': [0.48828125], 'Power_Band1': ['0.73-4.15'], 'Power_Band2': ['4.15-12.45'], 'Power_Band3': ['132.08-134.03'], 'Power_Band4': ['128.17-129.64'], 'Power_Band5': ['0.73-4.15'], 'Power_Band6': ['4.64-12.45'], 'Power_Band7': ['17.82-30.03'], 'Power_Band8': ['39.79-63.23'], 'Power_Band1_indices': ['3  9'], 'Power_Band2_indices': ['10  26'], 'Power_Band3_indices': ['272  275'], 'Power_Band4_indices': ['264  266'], 'Power_Band5_indices': ['3  9'], 'Power_Band6_indices': ['11  26'], 'Power_Band7_indices': ['38  62'], 'Power_Band8_indices': ['83  130'], 'Power_Band1_bins': ['0.98-3.91'], 'Power_Band2_bins': ['4.39-12.21'], 'Power_Band3_bins': ['132.32-133.79'], 'Power_Band4_bins': ['128.42-129.39'], 'Power_Band5_bins': ['0.98-3.91'], 'Power_Band6_bins': ['4.88-12.21'], 'Power_Band7_bins': ['18.07-29.79'], 'Power_Band8_bins': ['40.04-62.99']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 13:18:10,717\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1330970, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/1551955877.py\", line 37, in hyperparameter_search_pipeline\n",
      "  File \"/tmp/ipykernel_1320946/4223970823.py\", line 39, in add_simulated_ffts\n",
      "  File \"/tmp/ipykernel_1320946/4223970823.py\", line 34, in fft_sim_row\n",
      "IndexError: list index out of range\n",
      "2023-04-04 13:18:40,727\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1330959, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/1551955877.py\", line 37, in hyperparameter_search_pipeline\n",
      "  File \"/tmp/ipykernel_1320946/4223970823.py\", line 39, in add_simulated_ffts\n",
      "  File \"/tmp/ipykernel_1320946/4223970823.py\", line 34, in fft_sim_row\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m [231, 234]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m {'samplingRate': [500], 'gain_1': [232], 'gain_2': [231], 'gain_3': [231], 'gain_4': [234], 'fft_bandFormationConfig': [3], 'fft_interval': [1000], 'fft_size': [1024], 'fft_windowLoad': ['100% Hann'], 'fft_numBins': [512], 'fft_binWidth': [0.48828125], 'Power_Band1': ['0.73-4.15'], 'Power_Band2': ['4.64-12.45'], 'Power_Band3': ['132.08-134.03'], 'Power_Band4': ['128.17-129.64'], 'Power_Band5': ['0.73-4.15'], 'Power_Band6': ['4.64-12.45'], 'Power_Band7': ['17.82-30.03'], 'Power_Band8': ['39.79-63.23'], 'Power_Band1_indices': ['3  9'], 'Power_Band2_indices': ['11  26'], 'Power_Band3_indices': ['272  275'], 'Power_Band4_indices': ['264  266'], 'Power_Band5_indices': ['3  9'], 'Power_Band6_indices': ['11  26'], 'Power_Band7_indices': ['38  62'], 'Power_Band8_indices': ['83  130'], 'Power_Band1_bins': ['0.98-3.91'], 'Power_Band2_bins': ['4.88-12.21'], 'Power_Band3_bins': ['132.32-133.79'], 'Power_Band4_bins': ['128.42-129.39'], 'Power_Band5_bins': ['0.98-3.91'], 'Power_Band6_bins': ['4.88-12.21'], 'Power_Band7_bins': ['18.07-29.79'], 'Power_Band8_bins': ['40.04-62.99']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m (4, 159301, 512)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m (159301, 5)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m [235, 233]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m (4, 159301, 512)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m (159301, 5)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m [235, 233]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m (4, 159301, 512)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m (159301, 5)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m [235, 233]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 13:24:40,816\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1330970, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/1551955877.py\", line 37, in hyperparameter_search_pipeline\n",
      "  File \"/tmp/ipykernel_1320946/3690921258.py\", line 42, in add_simulated_ffts\n",
      "  File \"/tmp/ipykernel_1320946/3690921258.py\", line 37, in fft_sim_row\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m (4, 181393, 512)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m (181393, 5)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m [231, 234]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m (4, 181393, 512)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m (181393, 5)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m [231, 234]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m (4, 181393, 512)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m (181393, 5)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m [231, 234]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 13:24:57,821\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1330959, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/1551955877.py\", line 37, in hyperparameter_search_pipeline\n",
      "  File \"/tmp/ipykernel_1320946/3690921258.py\", line 42, in add_simulated_ffts\n",
      "  File \"/tmp/ipykernel_1320946/3690921258.py\", line 37, in fft_sim_row\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m [231, 234]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m {'samplingRate': [500], 'gain_1': [232], 'gain_2': [231], 'gain_3': [231], 'gain_4': [234], 'fft_bandFormationConfig': [3], 'fft_interval': [1000], 'fft_size': [1024], 'fft_windowLoad': ['100% Hann'], 'fft_numBins': [512], 'fft_binWidth': [0.48828125], 'Power_Band1': ['0.73-4.15'], 'Power_Band2': ['4.64-12.45'], 'Power_Band3': ['132.08-134.03'], 'Power_Band4': ['128.17-129.64'], 'Power_Band5': ['0.73-4.15'], 'Power_Band6': ['4.64-12.45'], 'Power_Band7': ['17.82-30.03'], 'Power_Band8': ['39.79-63.23'], 'Power_Band1_indices': ['3  9'], 'Power_Band2_indices': ['11  26'], 'Power_Band3_indices': ['272  275'], 'Power_Band4_indices': ['264  266'], 'Power_Band5_indices': ['3  9'], 'Power_Band6_indices': ['11  26'], 'Power_Band7_indices': ['38  62'], 'Power_Band8_indices': ['83  130'], 'Power_Band1_bins': ['0.98-3.91'], 'Power_Band2_bins': ['4.88-12.21'], 'Power_Band3_bins': ['132.32-133.79'], 'Power_Band4_bins': ['128.42-129.39'], 'Power_Band5_bins': ['0.98-3.91'], 'Power_Band6_bins': ['4.88-12.21'], 'Power_Band7_bins': ['18.07-29.79'], 'Power_Band8_bins': ['40.04-62.99']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m ['SessionIdentity', 'SleepStage', 'DerivedTime', 'Power_Band1', 'Power_Band2', 'Power_Band5', 'Power_Band6', 'Power_Band7', 'Power_Band8', 'TD_key2', 'TD_key3', 'TD_count', 'localTime', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m ['TD_key2', 'TD_key3', 'TD_count', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m (4, 213661, 512)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m (213661, 5)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m [233, 231]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m (4, 213661, 512)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m (213661, 5)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m [233, 231]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m (4, 213661, 512)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m (213661, 5)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330959)\u001b[0m [233, 231]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 13:29:46,887\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1330959, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/3079146302.py\", line 38, in hyperparameter_search_pipeline\n",
      "  File \"/tmp/ipykernel_1320946/1746186864.py\", line 44, in add_simulated_ffts\n",
      "  File \"/tmp/ipykernel_1320946/1746186864.py\", line 39, in fft_sim_row\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m ['TD_key2', 'TD_key3', 'TD_count', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m (4, 181393, 512)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m (181393, 5)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m [231, 234]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m (4, 181393, 512)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m (181393, 5)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m [231, 234]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m (4, 181393, 512)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m (181393, 5)\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1330970)\u001b[0m [231, 234]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 13:30:18,896\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1330970, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/3079146302.py\", line 38, in hyperparameter_search_pipeline\n",
      "  File \"/tmp/ipykernel_1320946/1746186864.py\", line 44, in add_simulated_ffts\n",
      "  File \"/tmp/ipykernel_1320946/1746186864.py\", line 39, in fft_sim_row\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1331024)\u001b[0m Executing Device:  09L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-04 13:37:00,485 E 1330841 1330841] (raylet) node_manager.cc:3040: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 569141cbc5ebfde1dd31aa81dd7722ee96f640efb88b175823a975ce, IP: 10.37.129.11) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.37.129.11`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-04 13:37:00,999\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1330970, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/3079146302.py\", line 45, in hyperparameter_search_pipeline\n",
      "  File \"/tmp/ipykernel_1320946/380465518.py\", line 21, in get_fft_corrs\n",
      "  File \"/home/claysmyth/miniconda3/envs/sleepclass/lib/python3.10/site-packages/polars/internals/dataframe/frame.py\", line 6445, in select\n",
      "    self.lazy()\n",
      "  File \"/home/claysmyth/miniconda3/envs/sleepclass/lib/python3.10/site-packages/polars/internals/lazyframe/frame.py\", line 1438, in collect\n",
      "    return pli.wrap_df(ldf.collect())\n",
      "exceptions.ColumnNotFoundError: field_236\n",
      "\n",
      "Error originated just after this operation:\n",
      "DF [\"row_nr\", \"SessionIdentity\", \"SleepStage\", \"DerivedTime\"]; PROJECT */254 COLUMNS; SELECTION: \"None\"\n",
      "2023-04-04 13:37:14,003\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1330959, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/3079146302.py\", line 45, in hyperparameter_search_pipeline\n",
      "  File \"/tmp/ipykernel_1320946/380465518.py\", line 21, in get_fft_corrs\n",
      "  File \"/home/claysmyth/miniconda3/envs/sleepclass/lib/python3.10/site-packages/polars/internals/dataframe/frame.py\", line 6445, in select\n",
      "    self.lazy()\n",
      "  File \"/home/claysmyth/miniconda3/envs/sleepclass/lib/python3.10/site-packages/polars/internals/lazyframe/frame.py\", line 1438, in collect\n",
      "    return pli.wrap_df(ldf.collect())\n",
      "exceptions.ColumnNotFoundError: field_236\n",
      "\n",
      "Error originated just after this operation:\n",
      "DF [\"row_nr\", \"SessionIdentity\", \"SleepStage\", \"DerivedTime\"]; PROJECT */254 COLUMNS; SELECTION: \"None\"\n",
      "2023-04-04 13:40:34,614\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1361675, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/3079146302.py\", line 45, in hyperparameter_search_pipeline\n",
      "  File \"/tmp/ipykernel_1320946/380465518.py\", line 21, in get_fft_corrs\n",
      "  File \"/home/claysmyth/miniconda3/envs/sleepclass/lib/python3.10/site-packages/polars/internals/dataframe/frame.py\", line 6445, in select\n",
      "    self.lazy()\n",
      "  File \"/home/claysmyth/miniconda3/envs/sleepclass/lib/python3.10/site-packages/polars/internals/lazyframe/frame.py\", line 1438, in collect\n",
      "    return pli.wrap_df(ldf.collect())\n",
      "exceptions.ColumnNotFoundError: field_236\n",
      "\n",
      "Error originated just after this operation:\n",
      "DF [\"row_nr\", \"SessionIdentity\", \"SleepStage\", \"DerivedTime\"]; PROJECT */254 COLUMNS; SELECTION: \"None\"\n",
      "2023-04-04 13:47:29,708\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::hyperparameter_search_wrapper()\u001b[39m (pid=1361675, ip=10.37.129.11)\n",
      "  File \"/tmp/ipykernel_1320946/3663473140.py\", line 9, in hyperparameter_search_wrapper\n",
      "  File \"/tmp/ipykernel_1320946/2749215327.py\", line 49, in hyperparameter_search_pipeline\n",
      "  File \"/home/claysmyth/.local/lib/python3.10/site-packages/pandas/core/frame.py\", line 663, in __init__\n",
      "    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n",
      "  File \"/home/claysmyth/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py\", line 493, in dict_to_mgr\n",
      "    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n",
      "  File \"/home/claysmyth/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py\", line 118, in arrays_to_mgr\n",
      "    index = _extract_index(arrays)\n",
      "  File \"/home/claysmyth/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py\", line 666, in _extract_index\n",
      "    raise ValueError(\"All arrays must be of the same length\")\n",
      "ValueError: All arrays must be of the same length\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m /home/claysmyth/miniconda3/envs/sleepclass/lib/python3.10/site-packages/joblib/externals/loky/backend/resource_tracker.py:318: UserWarning: resource_tracker: There appear to be 6 leaked semlock objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m /home/claysmyth/miniconda3/envs/sleepclass/lib/python3.10/site-packages/joblib/externals/loky/backend/resource_tracker.py:318: UserWarning: resource_tracker: There appear to be 39110 leaked folder objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m /home/claysmyth/miniconda3/envs/sleepclass/lib/python3.10/site-packages/joblib/externals/loky/backend/resource_tracker.py:318: UserWarning: resource_tracker: There appear to be 6 leaked semlock objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m /home/claysmyth/miniconda3/envs/sleepclass/lib/python3.10/site-packages/joblib/externals/loky/backend/resource_tracker.py:318: UserWarning: resource_tracker: There appear to be 38165 leaked folder objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-04 20:58:39,401 E 1361528 1361528] (raylet) node_manager.cc:3040: 40 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 08fa26942c353f32aa75422977ae539993840b0a8c9f2035fa38d8d6, IP: 10.37.129.11) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.37.129.11`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-04 20:59:39,403 E 1361528 1361528] (raylet) node_manager.cc:3040: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 08fa26942c353f32aa75422977ae539993840b0a8c9f2035fa38d8d6, IP: 10.37.129.11) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.37.129.11`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "# Test if plotly displays\n",
    "px.box(pd.DataFrame({'A': np.arange(50), 'B': np.arange(50,100)}).melt(value_vars=['A', 'B']), x='variable', y='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Put this into script so can be accessed as a module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database='/media/shortterm_ssd/Clay/databases/duckdb/rcs-db.duckdb', read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_as_pl_df(device, db_con, lazy=False):\n",
    "    \"\"\"\n",
    "    Accesses duckdb database and returns columns of interest, labeled reasonably with session identifiers cast as categoricals.\n",
    "    :param device: device name (str) (e.g. '02L')\n",
    "    :param db_con: duckdb connection object\n",
    "    :return: polars Dataframe\n",
    "    \"\"\"\n",
    "    db_con.sql(\"SET TIMEZONE = 'America/Los_Angeles'\")\n",
    "    if lazy:\n",
    "        return db_con.sql(f\"select DerivedTime, columns('localTime'), columns('^Session|TD_|Power_Band'), SleepStage from overnight.r{device}\").pl().lazy().with_columns(\n",
    "            pl.col('^Session.*$').cast(pl.Categorical)\n",
    "        )\n",
    "    else:\n",
    "        return db_con.sql(f\"select DerivedTime, columns('localTime'), columns('^Session|TD_|Power_Band'), SleepStage from overnight.r{device}\").pl().with_columns(\n",
    "            pl.col('^Session.*$').cast(pl.Categorical)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_as_pd_df(device, db_con):\n",
    "    \"\"\"\n",
    "    Accesses duckdb database and returns columns of interest, labeled reasonably with session identifiers cast as categoricals.\n",
    "    :param device: device name (str) (e.g. '02L')\n",
    "    :param db_con: duckdb connection object\n",
    "    :return: polars Dataframe\n",
    "    \"\"\"\n",
    "    return db_con.execute(f\"select DerivedTime, columns('localTime'), columns('^Session|TD_|Power_Band'), SleepStage from overnight.r{device}\").df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCS_SIM Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rcssim_wrapper(td_data, times, settings, gain) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    This function takes in polars lists of times (DerivedTimes) and timedomain data (td_data), and returns an estimate of the embedded RCS fft vector.\n",
    "    params:\n",
    "    times: (pl.list of floats: unix timestamps) 1xn vector of DerivedTimes\n",
    "    td_data: (pl.list of floats: millivolts) 1xn vector of time domain data\n",
    "    settings: (dict) settings for device\n",
    "    returns: data_fft_pl: (pl.list of floats) Estimate of embedded FFT vector(s) (size: mxn) corresponding to td_data\n",
    "             t_pb: (float: unix timestamp) DerivedTime unix timestamp of FFT vector\n",
    "    \"\"\"\n",
    "    hann_win = rcs.create_hann_window(settings['fft_size'][0], percent=100)\n",
    "    # times_np = times.to_numpy(zero_copy_only=True)\n",
    "\n",
    "    # td_np = rcs.transform_mv_to_rcs(td_data.to_numpy(zero_copy_only=True), gain)\n",
    "    td_np = rcs.transform_mv_to_rcs(td_data, gain)\n",
    "\n",
    "    data_fft, t_pb = rcs.td_to_fft(td_np, times,\n",
    "                                   settings['samplingRate'][0],\n",
    "                                   settings['fft_size'][0], settings['fft_interval'][0],\n",
    "                                   hann_win, interp_drops=False, output_in_mv=False, shift_timestamps_up_by_one_ind=True)\n",
    "    data_fft_out = rcs.fft_to_pb(data_fft, settings['samplingRate'][0], settings['fft_size'][0],\n",
    "                                 settings['fft_bandFormationConfig'][0],\n",
    "                                 input_is_mv=False)\n",
    "    return data_fft_out, t_pb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gains_from_settings_dict(settings, sessions, identity_col, device, db_con) -> np.ndarray:\n",
    "    \"\"\"\n",
    "\n",
    "    :param settings:\n",
    "    :param sessions:\n",
    "    :param identity_col:\n",
    "    :param device:\n",
    "    :param db_con:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    td_settings = db_con.execute(f\"Select columns('^chan'), columns('^gain') from r{device}.TDSettings where {identity_col} in {*sessions,}\").df().drop_duplicates()\n",
    "    if len(td_settings) > 1:\n",
    "        print(f\"WARNING: MULTIPLE TIME DOMAIN SETTINGS FOUND for {device} sessions {sessions}. Using the first unique values.\")\n",
    "    gains = []\n",
    "    for i in range(1,len(td_settings.columns)+1):\n",
    "        if (f\"chan{i}\" in td_settings.columns) and not ('Disabled' in td_settings[f\"chan{i}\"][0]):\n",
    "            gains.append(td_settings[f\"gain_{i}\"][0])\n",
    "    return np.array(gains)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_settings_for_pb_calcs(device, db_con, session_nums, identity_col):\n",
    "    \"\"\"\n",
    "    Get the relevant settings to run rcs_sim package functions:\n",
    "        rcs.create_hann_window\n",
    "        rcs.transform_mv_to_rcs\n",
    "        rcs.td_to_fft\n",
    "        rcs.fft_to_pb\n",
    "\n",
    "    :param device:\n",
    "    :param db_con:\n",
    "    :param session_nums: list of SessionIdentity values to pull settings from\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # TODO: FIX THIS FUNCTION TO ALLOW FOR identity_col = 'Session#'\n",
    "\n",
    "    td_settings = db_con.execute(f\"Select {identity_col}, samplingRate, columns('^gain') from r{device}.TDSettings where {identity_col} in {*session_nums,}\").df()\n",
    "    fft_settings = db_con.execute(f\"Select {identity_col}, fft_bandFormationConfig, fft_interval, fft_size, fft_windowLoad, fft_numBins, fft_binWidth, columns('^Power_Band') from r{device}.FftAndPowerSettings where SessionIdentity in {*session_nums,}\").df()\n",
    "    settings_df = pd.concat([td_settings, fft_settings], axis=1, join='inner')\n",
    "    settings_df['fft_bandFormationConfig'] = settings_df['fft_bandFormationConfig'].str.extract('(\\d+)').astype(int)\n",
    "    if not (np.sum(np.unique(settings_df.loc[:, settings_df.columns!=identity_col].nunique().values)) == 1):\n",
    "        print(\"WARNING: Session numbers provided have different settings.\")\n",
    "        print('Settings dataframe:')\n",
    "        print(settings_df)\n",
    "\n",
    "    return settings_df.loc[:, settings_df.columns!=identity_col].drop_duplicates().to_dict('list')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline Development\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIPELINE (each step will be an aggregate of functions)\n",
    "\n",
    "  1. Collect Patient Data\n",
    "  2. Transform into TD Windows\n",
    "  For Shift value\n",
    "  3. Collect FFTs via RCS_SIM package\n",
    "  4. COLLECT PBs\n",
    "\n",
    "Additionally,\n",
    "  1. Split FFT vectors into sections based on correlation (or mutual information) values, magnitude needs to be above threshold\n",
    "  1. Create Grid of hyperparameters\n",
    "  2. Train and cross validate each of the below models on data collected above\\\n",
    "    *LDA\\\n",
    "    *2-step LDA\\\n",
    "    *Decision Tree\n",
    "\n",
    "Be careful to keep track of how data was partitioned in cross-validation, so that models can easily be recreated.\n",
    "\n",
    "Save Grid and Classification Scores in pl Dataframe, and export to parquet or DuckDB (or both)\n",
    "\n",
    "After the best hyperparameters have been identified, use ALL data to train a final model for production.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_stage_mapping = {2: 1, 3: 1, 4: 0, 5: 0, 6: 0}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_df_by_timesegment_deprecated(df, interval='1s', period='2s', sample_rate=500, align_with_PB_outputs=False):\n",
    "    \"\"\"\n",
    "    Chunk a dataframe into smaller dataframes based on a time interval and period.\n",
    "    The period is the length of the time segment, the interval is the time between the start of each time segment.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataframe to be chunked\n",
    "    interval (str): The time interval between the start of each time segment. Default is '1s'\n",
    "    period (str): The length of each time segment. Default is '2s'\n",
    "    align_with_PB_outputs (bool): If True, the time segments will be aligned with the Power Band outputs. Default is False.\n",
    "    \"\"\"\n",
    "    # TODO: Remove hardcoding of 'SleepStage', should refer to it as a variable\n",
    "    if align_with_PB_outputs:\n",
    "        df_pb_count = df.join(\n",
    "            df.filter(pl.col('Power_Band8').is_not_null()).select(\n",
    "                'DerivedTime').with_row_count(),\n",
    "            on='DerivedTime', how='left').with_columns(pl.col('row_nr').fill_null(strategy='backward')).rename({'row_nr': 'PB_count'})\n",
    "\n",
    "        df_pb_count = df_pb_count.with_columns([\n",
    "            pl.when( (pl.col('PB_count') % 2) == 0).then(pl.lit(None)).otherwise(pl.col('PB_count')).fill_null(strategy='backward').alias('PB_count_odd'),\n",
    "            pl.when( (pl.col('PB_count') % 2) == 1).then(pl.lit(None)).otherwise(pl.col('PB_count')).fill_null(strategy='backward').alias('PB_count_even')\n",
    "        ])\n",
    "\n",
    "        df_pb_count = df_pb_count.groupby(['SleepStage', 'PB_count_even']).agg(\n",
    "            [\n",
    "                pl.col('DerivedTime'),\n",
    "                pl.col('^Power_Band.*$').drop_nulls().first(),\n",
    "                pl.col('TD_BG'),\n",
    "                pl.col('TD_key2'),\n",
    "                pl.col('TD_key3'),\n",
    "                pl.col('TD_BG').count().alias('TD_count')\n",
    "            ]).rename({'PB_count_even': 'PB_ind'}).vstack(\n",
    "                df_pb_count.groupby(['SleepStage', 'PB_count_odd']).agg(\n",
    "                    [\n",
    "                        pl.col('DerivedTime'),\n",
    "                        pl.col('^Power_Band.*$').drop_nulls().first(),\n",
    "                        pl.col('TD_BG'),\n",
    "                        pl.col('TD_key2'),\n",
    "                        pl.col('TD_key3'),\n",
    "                        pl.col('TD_BG').count().alias('TD_count')\n",
    "                    ]).rename({'PB_count_odd': 'PB_ind'})\n",
    "        ).select(pl.all().shrink_dtype()).rechunk()\n",
    "\n",
    "        df_chunked = df_pb_count\n",
    "    else:\n",
    "        df_grouped = df.sort('localTime').groupby_dynamic('localTime', every=interval, period=period, by=['SessionIdentity', 'SleepStage']).agg([\n",
    "            pl.col('DerivedTime'),\n",
    "            pl.col('^Power_Band.*$').drop_nulls().first(),\n",
    "            pl.col('TD_BG'),\n",
    "            pl.col('TD_key2'),\n",
    "            pl.col('TD_key3'),\n",
    "            pl.col('TD_BG').count().alias('TD_count')]).select(pl.all().shrink_dtype())\n",
    "\n",
    "        df_grouped = df_grouped.with_columns(\n",
    "                    pl.col('TD_BG').arr.eval(pl.element().is_null().any()).alias('TD_null')\n",
    "                ).filter((pl.col('TD_count') == int(period[0]) * sample_rate ) &\n",
    "                        (pl.col('TD_null').arr.contains(False))\n",
    "                        )\n",
    "        df_chunked = df_grouped\n",
    "    return df_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_df_by_timesegment(df, interval='1s', period='2s', sample_rate=500, align_with_PB_outputs=False, td_columns=['TD_BG', 'TD_key2', 'TD_key3']):\n",
    "    \"\"\"\n",
    "    Chunk a dataframe into smaller dataframes based on a time interval and period.\n",
    "    The period is the length of the time segment, the interval is the time between the start of each time segment.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataframe to be chunked\n",
    "    interval (str): The time interval between the start of each time segment. Default is '1s'\n",
    "    period (str): The length of each time segment. Default is '2s'\n",
    "    align_with_PB_outputs (bool): If True, the time segments will be aligned with the Power Band outputs. Default is False.\n",
    "    \"\"\"\n",
    "    td_cols = [col for col in df.columns if col in td_columns]\n",
    "    # TODO: Remove hardcoding of 'SleepStage', should refer to it as a variable\n",
    "    if align_with_PB_outputs:\n",
    "        df_pb_count = df.join(\n",
    "            df.filter(pl.col('Power_Band8').is_not_null()).select(\n",
    "                'DerivedTime').with_row_count(),\n",
    "            on='DerivedTime', how='left').with_columns(pl.col('row_nr').fill_null(strategy='backward')).rename({'row_nr': 'PB_count'})\n",
    "\n",
    "        df_pb_count = df_pb_count.with_columns([\n",
    "            pl.when( (pl.col('PB_count') % 2) == 0).then(pl.lit(None)).otherwise(pl.col('PB_count')).fill_null(strategy='backward').alias('PB_count_odd'),\n",
    "            pl.when( (pl.col('PB_count') % 2) == 1).then(pl.lit(None)).otherwise(pl.col('PB_count')).fill_null(strategy='backward').alias('PB_count_even')\n",
    "        ])\n",
    "\n",
    "        df_pb_count = df_pb_count.groupby(['SleepStage', 'PB_count_even']).agg(\n",
    "            [\n",
    "                pl.col('DerivedTime'),\n",
    "                pl.col('^Power_Band.*$').drop_nulls().first(),\n",
    "                pl.col('^TD_.*$'),\n",
    "                pl.col(td_cols[0]).count().alias('TD_count')\n",
    "            ]).rename({'PB_count_even': 'PB_ind'}).vstack(\n",
    "                df_pb_count.groupby(['SleepStage', 'PB_count_odd']).agg(\n",
    "                    [\n",
    "                        pl.col('DerivedTime'),\n",
    "                        pl.col('^Power_Band.*$').drop_nulls().first(),\n",
    "                        pl.col('^TD_.*$'),\n",
    "                pl.col(td_cols[0]).count().alias('TD_count')\n",
    "                    ]).rename({'PB_count_odd': 'PB_ind'})\n",
    "        ).select(pl.all().shrink_dtype()).rechunk()\n",
    "\n",
    "        df_chunked = df_pb_count\n",
    "    else:\n",
    "        df_grouped = df.sort('localTime').groupby_dynamic('localTime', every=interval, period=period, by=['SessionIdentity', 'SleepStage']).agg([\n",
    "            pl.col('DerivedTime'),\n",
    "            pl.col('^Power_Band.*$').drop_nulls().first(),\n",
    "            pl.col('^TD_.*$'),\n",
    "                pl.col(td_cols[0]).count().alias('TD_count')]).select(pl.all().shrink_dtype())\n",
    "\n",
    "        df_grouped = df_grouped.with_columns(\n",
    "                    pl.col(td_cols[0]).arr.eval(pl.element().is_null().any()).alias('TD_null')\n",
    "                ).filter((pl.col('TD_count') == int(period[0]) * sample_rate ) &\n",
    "                        (pl.col('TD_null').arr.contains(False))\n",
    "                        )\n",
    "        df_chunked = df_grouped\n",
    "    return df_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_simulated_ffts_deprecated(df_chunked, settings, gains, shift=None):    \n",
    "    \"\"\"\n",
    "    Add simulated FFTs to a dataframe of time segments. This function calls the rcs_sim package to simulate RC+S outputs\n",
    "    \n",
    "    parameters:\n",
    "    df_chunked (pl.DataFrame): The dataframe of time segments, as output by chunk_df_by_timesegment\n",
    "    settings (dict): The settings dictionary for the simulation (usually replicating the settings used to generate the original FFTs from an RCS session)\n",
    "    gains (list): The amp gains to use for the simulation\n",
    "    fft_subset_inds (list): The indices of the FFTs to use for the simulation. Default is [2,120]\n",
    "\n",
    "    returns:\n",
    "    df_chunked (pl.DataFrame): The dataframe of time segments with simulated FFTs added as additional columns. Each time domain channel has its own set of simulated FFTs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select the time domain channels and timestamps to use for the simulating FFTs\n",
    "    td_np = df_chunked.select([\n",
    "        pl.col('DerivedTime'),\n",
    "        pl.col('TD_BG'),\n",
    "        pl.col('TD_key2'),\n",
    "        pl.col('TD_key3')\n",
    "    ]).collect().to_numpy()\n",
    "\n",
    "    fft_arr_BG = np.zeros((td_np.shape[0], settings['fft_numBins'][0]))\n",
    "    fft_arr_2 = np.zeros((td_np.shape[0], settings['fft_numBins'][0]))\n",
    "    fft_arr_3 = np.zeros((td_np.shape[0], settings['fft_numBins'][0]))\n",
    "\n",
    "    sim_settings = settings.copy()\n",
    "    if shift:\n",
    "        sim_settings['fft_bandFormationConfig'] = [shift]\n",
    "\n",
    "\n",
    "    # Could try to use numba to speed up the loop, or use 'pl.apply' on the dataframe\n",
    "    # Simulate FFTs for each time segment, one FFT period at a time\n",
    "\n",
    "    # This is a horrible way to access gains. Need to make parameterize it and verify which subcortical gain is being used...\n",
    "    def fft_sim_row(i):\n",
    "        fft_arr_BG[i], _ = rcssim_wrapper(td_np[i,1], td_np[i,0], sim_settings, gains[0])\n",
    "        fft_arr_2[i], _ = rcssim_wrapper(td_np[i,2],td_np[i,0],  sim_settings, gains[1])\n",
    "        fft_arr_3[i], _ = rcssim_wrapper(td_np[i,3],td_np[i,0],  sim_settings, gains[2])\n",
    "\n",
    "\n",
    "    #@njit(parallel=True)\n",
    "    for i in range(td_np.shape[0]):\n",
    "        fft_sim_row(i)\n",
    "\n",
    "    # df_chunked = df_chunked.hstack(\n",
    "    # pl.DataFrame({'fft_BG': fft_arr_BG,\n",
    "    #                 'fft_key2': fft_arr_2,\n",
    "    #                 'fft_key3': fft_arr_3})\n",
    "    # )\n",
    "\n",
    "    # Join the simulated FFTs to the dataframe\n",
    "    df_chunked = df_chunked.with_row_count().join(\n",
    "                            pl.LazyFrame({'fft_BG': fft_arr_BG,\n",
    "                                            'fft_key2': fft_arr_2,\n",
    "                                            'fft_key3': fft_arr_3}).with_row_count(), \n",
    "                            on='row_nr', how='left')\n",
    "\n",
    "    return df_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_simulated_ffts(df_chunked, settings, gains, shift=None, td_columns=['TD_BG', 'TD_key2', 'TD_key3']):    \n",
    "    \"\"\"\n",
    "    Add simulated FFTs to a dataframe of time segments. This function calls the rcs_sim package to simulate RC+S outputs\n",
    "    \n",
    "    parameters:\n",
    "    df_chunked (pl.DataFrame): The dataframe of time segments, as output by chunk_df_by_timesegment\n",
    "    settings (dict): The settings dictionary for the simulation (usually replicating the settings used to generate the original FFTs from an RCS session)\n",
    "    gains (list): The amp gains to use for the simulation\n",
    "    fft_subset_inds (list): The indices of the FFTs to use for the simulation. Default is [2,120]\n",
    "\n",
    "    returns:\n",
    "    df_chunked (pl.DataFrame): The dataframe of time segments with simulated FFTs added as additional columns. Each time domain channel has its own set of simulated FFTs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select the time domain channels and timestamps to use for the simulating FFTs\n",
    "    td_np = df_chunked.select([\n",
    "        pl.col('DerivedTime'),\n",
    "        pl.col('^TD_.*$')\n",
    "    ]).collect().to_numpy()\n",
    "\n",
    "    td_cols = [col for col in df_chunked.columns if col in td_columns]\n",
    "\n",
    "    print(td_cols)\n",
    "\n",
    "    fft_arr = np.zeros((len(td_cols), td_np.shape[0], settings['fft_numBins'][0]))\n",
    "\n",
    "    sim_settings = settings.copy()\n",
    "    if shift:\n",
    "        sim_settings['fft_bandFormationConfig'] = [shift]\n",
    "\n",
    "\n",
    "    # Could try to use numba to speed up the loop, or use 'pl.apply' on the dataframe\n",
    "    # Simulate FFTs for each time segment, one FFT period at a time\n",
    "    def fft_sim_row(i):\n",
    "        for j in range(len(td_cols)):\n",
    "            fft_arr[j,i], _ = rcssim_wrapper(td_np[i,j+1], td_np[i,0], sim_settings, gains[j])\n",
    "\n",
    "\n",
    "    #@njit(parallel=True)\n",
    "    for i in range(td_np.shape[0]):\n",
    "        fft_sim_row(i)\n",
    "\n",
    "    # Join the simulated FFTs to the dataframe\n",
    "    df_chunked = df_chunked.with_row_count().join(\n",
    "                            pl.LazyFrame({f'fft_{i}': fft_arr[i] for i in range(len(td_cols))}).with_row_count(), \n",
    "                            on='row_nr', how='left')\n",
    "\n",
    "    return df_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_fft_arrays_with_subset_deprecated(df, fft_ind_start, vec_length):\n",
    "    \"\"\"\n",
    "    Take a subset of each Time Domain Channels FFT simulations, and expand each fft bin into it's own column\n",
    "\n",
    "    parameters:\n",
    "    df (pl.DataFrame): The dataframe of time segments with simulated FFTs added as additional columns. Each time domain channel has its own set of simulated FFTs.\n",
    "    fft_ind_start (int): The index of the first FFT bin to use\n",
    "    vec_length (int): The number of FFT bins to use when calculating the fft subset\n",
    "\n",
    "    returns:\n",
    "    df (pl.DataFrame): The dataframe of time segments with simulated FFTs bins added as individual columns\n",
    "    \"\"\"\n",
    "    return (df.with_columns([\n",
    "                        pl.col('^fft_.*$').arr.slice(fft_ind_start, vec_length)\n",
    "                    ])\n",
    "                    .with_columns(\n",
    "                        pl.col('fft_BG').arr.concat([pl.col('fft_key2'), pl.col('fft_key3')]).arr.to_struct().alias('fft_vec')\n",
    "                    )\n",
    "                    .unnest('fft_vec')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_fft_arrays_with_subset(df, fft_ind_start, vec_length):\n",
    "    \"\"\"\n",
    "    Take a subset of each Time Domain Channels FFT simulations, and expand each fft bin into it's own column\n",
    "\n",
    "    parameters:\n",
    "    df (pl.DataFrame): The dataframe of time segments with simulated FFTs added as additional columns. Each time domain channel has its own set of simulated FFTs.\n",
    "    fft_ind_start (int): The index of the first FFT bin to use\n",
    "    vec_length (int): The number of FFT bins to use when calculating the fft subset\n",
    "\n",
    "    returns:\n",
    "    df (pl.DataFrame): The dataframe of time segments with simulated FFTs bins added as individual columns\n",
    "    \"\"\"\n",
    "    fft_cols = [col for col in df.columns if 'fft' in col]\n",
    "    return (df.with_columns([\n",
    "                        pl.col('^fft_.*$').arr.slice(fft_ind_start, vec_length)\n",
    "                    ])\n",
    "                    .with_columns(\n",
    "                        pl.col(fft_cols[0]).arr.concat(pl.col(fft_cols[1:])).arr.to_struct().alias('fft_vec')\n",
    "                    )\n",
    "                    .unnest('fft_vec')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft_corrs(df_chunked, fft_subset_inds=[2,120], label_col='SleepStage', td_columns=['TD_BG', 'TD_key2', 'TD_key3']) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the correlation between each FFT bin and the label columns (SleepStage, or other)\n",
    "\n",
    "    parameters:\n",
    "    df_chunked (pl.DataFrame): The dataframe of time segments with simulated FFTs added as additional columns. Each time domain channel has its own set of simulated FFTs.\n",
    "    fft_subset_inds (list): The indices of the subset of each FFT output to use for the simulation. Default is [2,120]\n",
    "    label_col (str): The column to use for the label, for which each frequency bin will be compared to in a pearson correlation measure. Default is 'SleepStage'\n",
    "\n",
    "    returns:\n",
    "    df_fft_vec (pl.DataFrame): The dataframe with the desired subset of simulated FFTs bins for each FFT interval added as individual columns\n",
    "    fft_corrs (np.ndarray): The correlation between each FFT bin and the label column\n",
    "    \"\"\"\n",
    "\n",
    "    fft_ind_start = fft_subset_inds[0]\n",
    "    vec_length = fft_subset_inds[1] - fft_subset_inds[0]\n",
    "\n",
    "    df_fft_vec = expand_fft_arrays_with_subset(df_chunked, fft_ind_start=fft_ind_start, vec_length=vec_length)\n",
    "\n",
    "    # Could convert fft_corrs to be a pl.dataframe with discord trick by @ms. This would allow it to be lazy and save on memory.\n",
    "    fft_corrs = df_fft_vec.select([\n",
    "                        pl.corr(f\"field_{i}\", pl.col(label_col).cast(pl.Float64), method='pearson').alias(f\"field_{i}_corr\") for i in range(vec_length*len(td_columns))\n",
    "                    ]).to_numpy().squeeze()\n",
    "    \n",
    "    return df_fft_vec, fft_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_by_corr_threshold(df, fft_corrs, corr_threshold, label_col='SleepStageBinary'):\n",
    "    \"\"\"\n",
    "    Reduce features for powerband selection, by selecting the FFT bins that have a correlation above the threshold\n",
    "\n",
    "    parameters:\n",
    "    df (pl.DataFrame): The dataframe of time segments (i.e. individual fft windows are single rows) with simulated FFTs added as additional columns.\n",
    "    corr_threshold (float): The threshold for the correlation between the FFT bins and the label column. Only FFT bins with a correlation above this threshold will kept for future feature selection.\n",
    "    label_col (str): The column to use for the label, for which each frequency bin will be compared to in a pearson correlation measure. Default is 'SleepStageBinary'\n",
    "    \"\"\"\n",
    "    X = df.select(pl.col(\"^field_.*$\")).to_numpy()\n",
    "    feature_group = np.argwhere(np.abs(fft_corrs) > corr_threshold)\n",
    "    X = X[:, np.where(np.abs(fft_corrs) > corr_threshold)[0]]\n",
    "    y = df.select(pl.col(label_col)).to_numpy().squeeze()\n",
    "    return feature_group, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PB_combinations_deprecated(sfs, feature_group, max_clusters=8):\n",
    "    PB_groupings = []\n",
    "    for i in range(2, max_clusters+1): \n",
    "        kmeans = KMeans(n_clusters=i, random_state=0).fit(sfs.get_support(indices=True)[:, np.newaxis])\n",
    "\n",
    "        pbs = [[feature_group[np.argwhere(kmeans.labels_ == j).squeeze()].min(), \n",
    "                feature_group[np.argwhere(kmeans.labels_ == j).squeeze()].max()] for j in range(i)]\n",
    "        \n",
    "        cluster_df = pd.DataFrame({'x':sfs.get_support(indices=True), 'y':np.zeros(sfs.get_support(indices=True).shape[0]), 'color':kmeans.labels_})\n",
    "        if i == 2:\n",
    "            chart = alt.Chart(cluster_df).mark_point().encode(x='x', y='y', color=alt.Color('color', scale=alt.Scale(scheme='category20b')))\n",
    "        else:\n",
    "            chart &= alt.Chart(cluster_df).mark_point().encode(x='x', y='y', color=alt.Color('color', scale=alt.Scale(scheme='category20b')))\n",
    "        \n",
    "        PB_groupings.append(pbs)\n",
    "    \n",
    "    pb_combos = []\n",
    "    for pbs in PB_groupings:\n",
    "        if len(pbs) <= 4:\n",
    "            pb_combos.append(pbs)\n",
    "        else:\n",
    "            pb_combos.append(list(combinations(pbs, 4)))\n",
    "            \n",
    "    return pb_combos, chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PB_combinations(features, max_clusters=8):\n",
    "    \"\"\"\n",
    "    Takes in the features and returns the possible powerband combinations. Features are fft indices as determined by SequentialFeatureSelector.\n",
    "\n",
    "    parameters:\n",
    "    features (np.ndarray): The fft indices that were selected by SequentialFeatureSelector. They chould correspond to the indicies within the entire fft vector (i.e. the three channels fft outputs horizontally concatenated), \n",
    "        not the subset of the FFTs that was used for feature selection.\n",
    "    max_clusters (int): The maximum number of possible powerbands to consider for selection. Default is 8.\n",
    "\n",
    "    returns:\n",
    "    pb_combos (list): A list of lists of lists. The first list is the combination of powerbands for the corresponding cluster amount, the second list is the individual powerbands, \n",
    "    and the third list is the start and end indices of the powerband.\n",
    "    chart (altair.Chart): An interactive chart of the powerband combinations. The x-axis is the fft indices, the y-axis is irrelevant, and the color is the cluster number.\n",
    "    \"\"\"\n",
    "    PB_groupings = []\n",
    "    for i in range(2, max_clusters+1): \n",
    "        kmeans = KMeans(n_clusters=i, random_state=0).fit(features[:, np.newaxis])\n",
    "\n",
    "        # Extract first and last index of each cluster, to be used as powerband boundaries\n",
    "        pbs = [[features[np.argwhere(kmeans.labels_ == j).squeeze()].min(), \n",
    "                features[np.argwhere(kmeans.labels_ == j).squeeze()].max()] for j in range(i)]\n",
    "        \n",
    "        cluster_df = pd.DataFrame({'x':features, 'y':np.zeros(features.shape[0]), 'color':kmeans.labels_})\n",
    "        base = alt.Chart(cluster_df).mark_point().encode(x='x', y='y', color=alt.Color('color', scale=alt.Scale(scheme='category20b'))).interactive()\n",
    "        if i == 2:\n",
    "            chart = base\n",
    "        else:\n",
    "            chart &= base\n",
    "        \n",
    "        PB_groupings.append(pbs)\n",
    "    \n",
    "    pb_combos = []\n",
    "    for pbs in PB_groupings:\n",
    "        if len(pbs) <= 4:\n",
    "            pb_combos.append(pbs)\n",
    "        else:\n",
    "            # Get all combinations of 4 powerbands chosen from the possible powerbands in that cluster (e.g. if num clusters is 8, add all combinations of 4 powerbands from the 8 powerbands)\n",
    "            pb_combos.append(list(combinations(pbs, 4)))\n",
    "            \n",
    "    return pb_combos, chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_original_feature_inds(feature_list, fft_subset_inds, fft_length):\n",
    "    \"\"\"\n",
    "    Recover the original fft indices (i.e. the indices of the fft bins from the FFT vector that corresponds to the concatenated simulations of each TD channel) from the feature list. This typically refers to putative powerband edges. \n",
    "    Recall the features were collected from the fft subset indices.\n",
    "\n",
    "    parameters:\n",
    "    feature_list (list): The list of features.\n",
    "    fft_subset_inds (list): The start and end indices of the fft subset that was used for feature selection.\n",
    "    fft_length (int): The length of the simulated fft vector for each TD channel (e.g. 512 for an FFT window of 1024 on a 500 TD sampling rate).\n",
    "\n",
    "    returns:\n",
    "    list: The list of original fft indices.\n",
    "    \"\"\"\n",
    "    vec_length = fft_subset_inds[1] - fft_subset_inds[0]\n",
    "    vec_start = fft_subset_inds[0]\n",
    "\n",
    "    return [(\n",
    "        (i - (vec_length * (i // vec_length)) ) # remove channel offset of index\n",
    "        + vec_start # add the start index of the fft vector subset\n",
    "        + (i // vec_length) * fft_length) # recover channel offset of original fft vector\n",
    "        for i in feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_pb_combos(pb_combos):\n",
    "    tmp = [[list(sub_ele) for sub_ele in ele] for ele in pb_combos]\n",
    "    tmp2 = []\n",
    "    for ele in tmp:\n",
    "        if len(ele) <= 4:\n",
    "            _ = [ele.append([None,None]) for i in range(4-len(ele))]\n",
    "            tmp2.append(ele)\n",
    "        else:\n",
    "            [tmp2.append(sub_ele) for sub_ele in ele]\n",
    "    return pl.DataFrame(tmp2, schema=['PB1', 'PB2', 'PB3', 'PB4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data_for_LDA(df_fft, pbs, update_rate, label_col='SleepStageBinary'):\n",
    "   simulated_ffts = (\n",
    "    df_fft\n",
    "    .select(\n",
    "            [\n",
    "                pl.col('fft_vec').arr.slice(pbs[i][0], (pbs[i][1] - pbs[i][0] + 1)).arr.sum().alias(f'Power_Band{i+1}') \n",
    "                for i in range(len(pbs)) if pbs[i][0] is not None\n",
    "            ] + \n",
    "            [pl.col(label_col)]\n",
    "    )\n",
    "    .with_row_count()\n",
    "    .with_columns([\n",
    "        pl.col('row_nr') // update_rate\n",
    "    ])\n",
    "    .groupby(['row_nr'])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(f'Power_Band{i+1}').mean() \n",
    "            for i in range(len(pbs)) if pbs[i][0] is not None\n",
    "        ] + \n",
    "        [pl.col(label_col).last().alias(label_col)]\n",
    "      )\n",
    "   )\n",
    "   \n",
    "   X = simulated_ffts.select(pl.col(\"^Power_Band.*$\")).to_numpy()\n",
    "   y = simulated_ffts.select(pl.col(label_col)).to_numpy().squeeze()\n",
    "   return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_scorer = make_scorer(\n",
    "#     roc_auc_score, greater_is_better=True, needs_threshold=True\n",
    "# )\n",
    "\n",
    "# balanced_accuracy_scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "def custom_scorer(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred, normalize='true')\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    # Leads to different AUC score if simply calling string 'roc_auc' in cross_validate \n",
    "    # Maybe because this is because the calling the string 'roc_auc' will use the default scorer,\n",
    "    #  which only returns the scores on the test sets in the cross-validation?\n",
    "    auc = roc_auc_score(y, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y, y_pred)\n",
    "    # return {'tn': cm[0, 0], 'fp': cm[0, 1], 'fn': cm[1, 0], 'tp': cm[1, 1],\n",
    "    #         'accuracy': acc, 'auc': auc, 'balanced_accuracy': bal_acc}\n",
    "    return {'accuracy': acc, 'auc': auc, 'balanced_accuracy': bal_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_scorer(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred, normalize='true')\n",
    "    return {'tn': cm[0, 0], 'fp': cm[0, 1], \n",
    "            'fn': cm[1, 0], 'tp': cm[1, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hyperparameter_search_pipeline(device, parameters, sleep_stage_mapping, np_random_seed=0):\n",
    "    assert len(parameters['TD_Columns']) > 1, 'Must select at least 2 TD channels to run hyperparameter search pipeline.'\n",
    "\n",
    "    print('Executing Device: ', device)\n",
    "    np.random.seed(np_random_seed)\n",
    "    con = duckdb.connect(database='/media/shortterm_ssd/Clay/databases/duckdb/rcs-db.duckdb', read_only=True)\n",
    "    \n",
    "    df = get_device_as_pl_df(device, con, lazy=True)\n",
    "    ALL_TD_COLUMNS = ['TD_BG', 'TD_key2', 'TD_key3']\n",
    "    # Keep only desired TD columns\n",
    "    df = df.select(pl.all().exclude( list(set(ALL_TD_COLUMNS) - set(parameters['TD_Columns'])) ) )\n",
    "    print('Analyzing: ' + ', '.join(df.select(pl.col('^TD.*$')).columns))\n",
    "\n",
    "    sessions = df.select('SessionIdentity').unique().collect().to_dict(\n",
    "        as_series=False)['SessionIdentity']\n",
    "    settings = get_settings_for_pb_calcs(device, con, sessions, 'SessionIdentity')\n",
    "    gains = get_gains_from_settings_dict(\n",
    "        settings, sessions, 'SessionIdentity', device, con)\n",
    "    \n",
    "    gains_tmp = []\n",
    "    for channel in parameters['TD_Columns']:\n",
    "        if channel == 'TD_BG':\n",
    "            gains_tmp.append(gains[0])\n",
    "        if channel == 'TD_key2':\n",
    "            gains_tmp.append(gains[1])\n",
    "        if channel == 'TD_key3':\n",
    "            gains_tmp.append(gains[2])\n",
    "    gains = gains_tmp\n",
    "    print(gains)\n",
    "\n",
    "    print(settings)\n",
    "    print('ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0')\n",
    "    fft_length = settings['fft_numBins'][0]\n",
    "\n",
    "    df_chunked = chunk_df_by_timesegment(df)\n",
    "    print(df_chunked.columns) # here\n",
    "    print(f'Simulating FFTs for TD chunks of size {settings[\"fft_size\"][0] - settings[\"fft_size\"][0]%250} samples')\n",
    "    df_chunked = add_simulated_ffts(df_chunked, settings, gains)\n",
    "    df_chunked = df_chunked.with_columns(pl.col('SleepStage').map_dict(sleep_stage_mapping).alias('SleepStageBinary'))\n",
    "\n",
    "    NUM_TD_CHANNELS = len(parameters['TD_Columns'])\n",
    "\n",
    "    df_chunked = df_chunked.collect()\n",
    "\n",
    "    df_fft_subset, fft_corrs = get_fft_corrs(df_chunked, parameters['fft_subset_inds'], label_col='SleepStageBinary', td_columns=parameters['TD_Columns'])\n",
    "\n",
    "    \n",
    "    fft_subset_length = parameters['fft_subset_inds'][1] - parameters['fft_subset_inds'][0]\n",
    "    # corr_df = pd.DataFrame({'Hz': 0.48*(np.arange(fft_subset_length) + parameters['fft_subset_inds'][0]), 'BG': fft_corrs[:fft_subset_length], 'key2': fft_corrs[fft_subset_length:fft_subset_length*2], \n",
    "    #                         'key3': fft_corrs[fft_subset_length*2:]}).melt(id_vars='Hz', value_vars=['BG', 'key2', 'key3'], var_name='key', value_name='corr')\n",
    "    # corr_chart = alt.Chart(corr_df).mark_line().encode(x='Hz', y='corr', color='key').interactive()\n",
    "\n",
    "    first_feature_group, X, y = get_train_data_by_corr_threshold(df_fft_subset, fft_corrs, parameters['corr_threshold'], label_col='SleepStageBinary')\n",
    "    del(df_fft_subset)\n",
    "    model = LinearDiscriminantAnalysis(solver='svd')\n",
    "    print('Running Sequential Feature Selector...')\n",
    "    sfs = SequentialFeatureSelector(model, n_features_to_select=parameters['num_features_to_select'], direction='forward', n_jobs=6, cv=5, scoring=parameters['sfs_scoring'])\n",
    "    sfs.fit(X, y)\n",
    "\n",
    "\n",
    "    second_feature_group = first_feature_group[sfs.get_support(indices=True)]\n",
    "    second_feature_group_original_inds = np.array(recover_original_feature_inds(second_feature_group, parameters['fft_subset_inds'], fft_length)).squeeze()\n",
    "    pb_combos, pb_chart = get_PB_combinations(second_feature_group_original_inds, max_clusters=parameters['max_clusters'])\n",
    "    df_pbs_corrected = get_df_from_pb_combos(pb_combos)\n",
    "    #df_pbs_corrected = recover_original_feature_inds(df_pb_combos, parameters['fft_subset_inds'], fft_length)\n",
    "\n",
    "    df_pbs_corrected = df_pbs_corrected.join(pl.DataFrame({'UpdateRate': [2, 5, 10, 15, 30]}), how='cross')\n",
    "\n",
    "    fft_cols = [col for col in df_chunked.columns if 'fft' in col]\n",
    "    # df_chunked = df_chunked.select([\n",
    "    #     pl.col('SleepStage'),\n",
    "    #     pl.col('SleepStageBinary'),\n",
    "    #     pl.col('fft_BG').arr.concat([pl.col('fft_key2'), pl.col('fft_key3')]).alias('fft_vec')\n",
    "    # ])\n",
    "    df_chunked = df_chunked.select([\n",
    "        pl.col('SleepStage'),\n",
    "        pl.col('SleepStageBinary'),\n",
    "        pl.col(fft_cols[0]).arr.concat(pl.col(fft_cols[1:])).alias('fft_vec')\n",
    "    ])\n",
    "    try:\n",
    "        assert fft_length == df_chunked.select(\n",
    "                pl.col('fft_vec').arr.lengths()\n",
    "            ).unique().item() / NUM_TD_CHANNELS\n",
    "    except AssertionError:\n",
    "        print('fft_length is not equal to the length of the fft_vec column.')\n",
    "        print('fft_length: ', fft_length)\n",
    "        print('length of fft_vec column: ', df_chunked.select(\n",
    "                pl.col('fft_vec').arr.lengths()\n",
    "            ).unique().item() / NUM_TD_CHANNELS)\n",
    "\n",
    "\n",
    "    print('Searching over powerband combos...')\n",
    "    scores = {'test_accuracy': [], 'test_roc_auc': [], 'test_balanced_accuracy': [], 'test_recall': [], 'test_precision': [], 'test_tnr': []}\n",
    "    scores_stds = {'test_accuracy': [], 'test_roc_auc': [], 'test_balanced_accuracy': [], 'test_recall': [], 'test_precision': [], 'test_tnr': []}\n",
    "    for i in range(df_pbs_corrected.height):\n",
    "        pbs = [value for value in df_pbs_corrected.select(pl.exclude('UpdateRate'))[i].to_dicts()[0].values()]\n",
    "        X, y = get_training_data_for_LDA(df_chunked, pbs, df_pbs_corrected[i,'UpdateRate'], label_col='SleepStageBinary')\n",
    "\n",
    "        # NOTE: 'roc_auc' gets label predictions with clf.predict_proba(X)[:, 1], which allows more thresholds to be tested. \n",
    "        # clf.predict_proba(X)[:, 1] is the probability of the positive class (1). clf.predict_proba(X)[:, 0] is the probability of the negative class (0)\n",
    "        score_dict = {'accuracy': 'accuracy', 'roc_auc': 'roc_auc', 'balanced_accuracy': 'balanced_accuracy', 'recall': 'recall', 'precision': 'precision'}\n",
    "\n",
    "        # cv_results = cross_validate(model, X, y, cv=5,\n",
    "        #                   scoring=custom_scorer, n_jobs=5)\n",
    "        cv_results = cross_validate(model, X, y, cv=5,\n",
    "                        scoring=score_dict, n_jobs=5)\n",
    "        \n",
    "        [scores[k].extend([np.mean(v)]) for (k, v) in cv_results.items() if k in scores.keys()]\n",
    "        [scores_stds[k].extend([np.std(v)]) for (k, v) in cv_results.items() if k in scores_stds.keys()]\n",
    "\n",
    "        tnr = np.array(cv_results['test_balanced_accuracy']) * 2 - cv_results['test_recall']\n",
    "        scores['test_tnr'].extend([np.mean(tnr)])\n",
    "        scores_stds['test_tnr'].extend([np.std(tnr)])\n",
    "        \n",
    "\n",
    "\n",
    "    df_hyperparams = pl.concat([df_pbs_corrected, pl.DataFrame(scores).rename(\n",
    "        {'test_accuracy': 'Acc', 'test_roc_auc': 'AUC', 'test_balanced_accuracy': 'BalAcc', 'test_recall': 'TPR', 'test_precision': 'Precision', 'test_tnr': 'TNR'}),\n",
    "        pl.DataFrame(scores_stds).rename({'test_accuracy': 'Acc_std', 'test_roc_auc': 'AUC_std', 'test_balanced_accuracy': 'BalAcc_std', 'test_recall': 'TPR_std', 'test_precision': 'precision_std', 'test_tnr': 'TNR_std'})\n",
    "        ], how='horizontal')\n",
    "\n",
    "    BASE_PATH =  f'/media/shortterm_ssd/Clay/databases/Sleep_10day_with_autonomic/RCS{device}/'\n",
    "    df_hyperparams.write_parquet(BASE_PATH + 'hyperparameter_search_results_only_cortical.parquet')\n",
    "    # corr_chart.save(BASE_PATH + 'sleepstage_corr.png')\n",
    "\n",
    "    # return (device, df_hyperparams, corr_chart.properties(title=f'{device}'), pb_chart.properties(title=f'{device}'))\n",
    "    return (device, df_hyperparams, pb_chart.properties(title=f'{device}'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rationale for 24 cpus: Max batch is 4 devices, and each device will use 5 cores for the cross-validation steps. \n",
    "# Allocating 1 extra process for cross-validation allows for 6 cores per device.\n",
    "# According to the documentation, num_cpus provides a 'max number' of cpu cores for that remote task.\n",
    "\n",
    "# Reduced to 12 because I think I overworked the server and it automatically restarted due to overheating or overload lol\n",
    "@ray.remote(num_cpus=24) \n",
    "def hyperparameter_search_wrapper(devices, params, sleep_stage_mapping):\n",
    "    for device in devices:\n",
    "        hyperparameter_search_pipeline(device, params, sleep_stage_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Executing Device:  07R\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Executing Device:  02L\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m WARNING: Session numbers provided have different settings.\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Settings dataframe:\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m    SessionIdentity  samplingRate  ...  Power_Band7_bins  Power_Band8_bins\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m 0     07R_05-13-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m 1     07R_05-12-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m 2     07R_05-20-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m 3     07R_05-24-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m 4     07R_05-25-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m 5     07R_06-07-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m 6     07R_06-07-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m 7     07R_06-09-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m 8     07R_06-30-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m 9     07R_07-22-22           500  ...       80.08-99.61         7.81-9.77\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m 10    07R_07-26-22           500  ...       80.08-99.61         7.81-9.77\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m [11 rows x 37 columns]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m WARNING: MULTIPLE TIME DOMAIN SETTINGS FOUND for 07R sessions ['07R_05-24-22', '07R_06-09-22', '07R_05-12-22', '07R_05-13-22', '07R_07-22-22', '07R_05-25-22', '07R_06-07-22', '07R_06-30-22', '07R_07-26-22', '07R_05-20-22']. Using the first unique values.\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m [233, 233]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m {'samplingRate': [500, 500], 'gain_1': [234, 234], 'gain_2': [233, 233], 'gain_3': [233, 233], 'gain_4': [233, 233], 'fft_bandFormationConfig': [3, 3], 'fft_interval': [1000, 50], 'fft_size': [1024, 256], 'fft_windowLoad': ['100% Hann', '100% Hann'], 'fft_numBins': [512, 128], 'fft_binWidth': [0.48828125, 1.953125], 'Power_Band1': ['0.73-4.15', '4.88-24.41'], 'Power_Band2': ['4.64-12.45', '12.70-30.27'], 'Power_Band3': ['132.08-134.03', '4.88-28.32'], 'Power_Band4': ['128.17-129.64', '32.23-59.57'], 'Power_Band5': ['0.73-4.15', '10.74-28.32'], 'Power_Band6': ['4.64-12.45', '57.62-100.59'], 'Power_Band7': ['17.82-30.03', '79.10-100.59'], 'Power_Band8': ['39.79-63.23', '6.84-10.74'], 'Power_Band1_indices': ['3  9', '4  13'], 'Power_Band2_indices': ['11  26', '8  16'], 'Power_Band3_indices': ['272  275', '4  15'], 'Power_Band4_indices': ['264  266', '18  31'], 'Power_Band5_indices': ['3  9', '7  15'], 'Power_Band6_indices': ['11  26', '31  52'], 'Power_Band7_indices': ['38  62', '42  52'], 'Power_Band8_indices': ['83  130', '5  6'], 'Power_Band1_bins': ['0.98-3.91', '5.86-23.44'], 'Power_Band2_bins': ['4.88-12.21', '13.67-29.30'], 'Power_Band3_bins': ['132.32-133.79', '5.86-27.34'], 'Power_Band4_bins': ['128.42-129.39', '33.20-58.59'], 'Power_Band5_bins': ['0.98-3.91', '11.72-27.34'], 'Power_Band6_bins': ['4.88-12.21', '58.59-99.61'], 'Power_Band7_bins': ['18.07-29.79', '80.08-99.61'], 'Power_Band8_bins': ['40.04-62.99', '7.81-9.77']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ['SessionIdentity', 'SleepStage', 'DerivedTime', 'Power_Band1', 'Power_Band2', 'Power_Band5', 'Power_Band6', 'Power_Band7', 'Power_Band8', 'TD_key2', 'TD_key3', 'TD_count', 'localTime', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m [233, 231]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m {'samplingRate': [500], 'gain_1': [232], 'gain_2': [231], 'gain_3': [233], 'gain_4': [231], 'fft_bandFormationConfig': [3], 'fft_interval': [1000], 'fft_size': [1024], 'fft_windowLoad': ['100% Hann'], 'fft_numBins': [512], 'fft_binWidth': [0.48828125], 'Power_Band1': ['0.73-4.15'], 'Power_Band2': ['4.15-12.45'], 'Power_Band3': ['132.08-134.03'], 'Power_Band4': ['128.17-129.64'], 'Power_Band5': ['0.73-4.15'], 'Power_Band6': ['4.64-12.45'], 'Power_Band7': ['17.82-30.03'], 'Power_Band8': ['39.79-63.23'], 'Power_Band1_indices': ['3  9'], 'Power_Band2_indices': ['10  26'], 'Power_Band3_indices': ['272  275'], 'Power_Band4_indices': ['264  266'], 'Power_Band5_indices': ['3  9'], 'Power_Band6_indices': ['11  26'], 'Power_Band7_indices': ['38  62'], 'Power_Band8_indices': ['83  130'], 'Power_Band1_bins': ['0.98-3.91'], 'Power_Band2_bins': ['4.39-12.21'], 'Power_Band3_bins': ['132.32-133.79'], 'Power_Band4_bins': ['128.42-129.39'], 'Power_Band5_bins': ['0.98-3.91'], 'Power_Band6_bins': ['4.88-12.21'], 'Power_Band7_bins': ['18.07-29.79'], 'Power_Band8_bins': ['40.04-62.99']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ['SessionIdentity', 'SleepStage', 'DerivedTime', 'Power_Band1', 'Power_Band2', 'Power_Band5', 'Power_Band6', 'Power_Band7', 'Power_Band8', 'TD_key2', 'TD_key3', 'TD_count', 'localTime', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ['TD_key2', 'TD_key3']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ['TD_key2', 'TD_key3']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Running Sequential Feature Selector...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Running Sequential Feature Selector...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Searching over powerband combos...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Executing Device:  09L\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m [235, 233]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m {'samplingRate': [500], 'gain_1': [233], 'gain_2': [234], 'gain_3': [235], 'gain_4': [233], 'fft_bandFormationConfig': [3], 'fft_interval': [1000], 'fft_size': [1024], 'fft_windowLoad': ['100% Hann'], 'fft_numBins': [512], 'fft_binWidth': [0.48828125], 'Power_Band1': ['0.73-4.15'], 'Power_Band2': ['4.15-12.45'], 'Power_Band3': ['132.08-134.03'], 'Power_Band4': ['128.17-129.64'], 'Power_Band5': ['0.73-4.15'], 'Power_Band6': ['4.64-12.45'], 'Power_Band7': ['17.82-30.03'], 'Power_Band8': ['39.79-63.23'], 'Power_Band1_indices': ['3  9'], 'Power_Band2_indices': ['10  26'], 'Power_Band3_indices': ['272  275'], 'Power_Band4_indices': ['264  266'], 'Power_Band5_indices': ['3  9'], 'Power_Band6_indices': ['11  26'], 'Power_Band7_indices': ['38  62'], 'Power_Band8_indices': ['83  130'], 'Power_Band1_bins': ['0.98-3.91'], 'Power_Band2_bins': ['4.39-12.21'], 'Power_Band3_bins': ['132.32-133.79'], 'Power_Band4_bins': ['128.42-129.39'], 'Power_Band5_bins': ['0.98-3.91'], 'Power_Band6_bins': ['4.88-12.21'], 'Power_Band7_bins': ['18.07-29.79'], 'Power_Band8_bins': ['40.04-62.99']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ['SessionIdentity', 'SleepStage', 'DerivedTime', 'Power_Band1', 'Power_Band2', 'Power_Band5', 'Power_Band6', 'Power_Band7', 'Power_Band8', 'TD_key2', 'TD_key3', 'TD_count', 'localTime', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ['TD_key2', 'TD_key3']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Running Sequential Feature Selector...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Searching over powerband combos...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Executing Device:  02R\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m [230, 232]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m {'samplingRate': [500], 'gain_1': [232], 'gain_2': [232], 'gain_3': [230], 'gain_4': [232], 'fft_bandFormationConfig': [3], 'fft_interval': [1000], 'fft_size': [1024], 'fft_windowLoad': ['100% Hann'], 'fft_numBins': [512], 'fft_binWidth': [0.48828125], 'Power_Band1': ['0.73-4.15'], 'Power_Band2': ['4.64-12.45'], 'Power_Band3': ['132.08-134.03'], 'Power_Band4': ['128.17-129.64'], 'Power_Band5': ['0.73-4.15'], 'Power_Band6': ['4.64-12.45'], 'Power_Band7': ['17.82-30.03'], 'Power_Band8': ['39.79-63.23'], 'Power_Band1_indices': ['3  9'], 'Power_Band2_indices': ['11  26'], 'Power_Band3_indices': ['272  275'], 'Power_Band4_indices': ['264  266'], 'Power_Band5_indices': ['3  9'], 'Power_Band6_indices': ['11  26'], 'Power_Band7_indices': ['38  62'], 'Power_Band8_indices': ['83  130'], 'Power_Band1_bins': ['0.98-3.91'], 'Power_Band2_bins': ['4.88-12.21'], 'Power_Band3_bins': ['132.32-133.79'], 'Power_Band4_bins': ['128.42-129.39'], 'Power_Band5_bins': ['0.98-3.91'], 'Power_Band6_bins': ['4.88-12.21'], 'Power_Band7_bins': ['18.07-29.79'], 'Power_Band8_bins': ['40.04-62.99']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ['SessionIdentity', 'SleepStage', 'DerivedTime', 'Power_Band1', 'Power_Band2', 'Power_Band5', 'Power_Band6', 'Power_Band7', 'Power_Band8', 'TD_key2', 'TD_key3', 'TD_count', 'localTime', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ['TD_key2', 'TD_key3']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Running Sequential Feature Selector...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Searching over powerband combos...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Executing Device:  09R\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m [231, 232]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m {'samplingRate': [500], 'gain_1': [231], 'gain_2': [231], 'gain_3': [231], 'gain_4': [232], 'fft_bandFormationConfig': [3], 'fft_interval': [1000], 'fft_size': [1024], 'fft_windowLoad': ['100% Hann'], 'fft_numBins': [512], 'fft_binWidth': [0.48828125], 'Power_Band1': ['0.73-4.15'], 'Power_Band2': ['4.64-12.45'], 'Power_Band3': ['132.08-134.03'], 'Power_Band4': ['128.17-129.64'], 'Power_Band5': ['0.73-4.15'], 'Power_Band6': ['4.64-12.45'], 'Power_Band7': ['17.82-30.03'], 'Power_Band8': ['39.79-63.23'], 'Power_Band1_indices': ['3  9'], 'Power_Band2_indices': ['11  26'], 'Power_Band3_indices': ['272  275'], 'Power_Band4_indices': ['264  266'], 'Power_Band5_indices': ['3  9'], 'Power_Band6_indices': ['11  26'], 'Power_Band7_indices': ['38  62'], 'Power_Band8_indices': ['83  130'], 'Power_Band1_bins': ['0.98-3.91'], 'Power_Band2_bins': ['4.88-12.21'], 'Power_Band3_bins': ['132.32-133.79'], 'Power_Band4_bins': ['128.42-129.39'], 'Power_Band5_bins': ['0.98-3.91'], 'Power_Band6_bins': ['4.88-12.21'], 'Power_Band7_bins': ['18.07-29.79'], 'Power_Band8_bins': ['40.04-62.99']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ['SessionIdentity', 'SleepStage', 'DerivedTime', 'Power_Band1', 'Power_Band2', 'Power_Band5', 'Power_Band6', 'Power_Band7', 'Power_Band8', 'TD_key2', 'TD_key3', 'TD_count', 'localTime', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ['TD_key2', 'TD_key3']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Running Sequential Feature Selector...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Searching over powerband combos...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Executing Device:  03L\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m [227, 231]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m {'samplingRate': [500], 'gain_1': [231], 'gain_2': [231], 'gain_3': [227], 'gain_4': [231], 'fft_bandFormationConfig': [3], 'fft_interval': [1000], 'fft_size': [1024], 'fft_windowLoad': ['100% Hann'], 'fft_numBins': [512], 'fft_binWidth': [0.48828125], 'Power_Band1': ['0.73-4.15'], 'Power_Band2': ['4.15-12.45'], 'Power_Band3': ['132.08-134.03'], 'Power_Band4': ['128.17-129.64'], 'Power_Band5': ['0.73-4.15'], 'Power_Band6': ['4.64-12.45'], 'Power_Band7': ['17.82-30.03'], 'Power_Band8': ['39.79-63.23'], 'Power_Band1_indices': ['3  9'], 'Power_Band2_indices': ['10  26'], 'Power_Band3_indices': ['272  275'], 'Power_Band4_indices': ['264  266'], 'Power_Band5_indices': ['3  9'], 'Power_Band6_indices': ['11  26'], 'Power_Band7_indices': ['38  62'], 'Power_Band8_indices': ['83  130'], 'Power_Band1_bins': ['0.98-3.91'], 'Power_Band2_bins': ['4.39-12.21'], 'Power_Band3_bins': ['132.32-133.79'], 'Power_Band4_bins': ['128.42-129.39'], 'Power_Band5_bins': ['0.98-3.91'], 'Power_Band6_bins': ['4.88-12.21'], 'Power_Band7_bins': ['18.07-29.79'], 'Power_Band8_bins': ['40.04-62.99']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ['SessionIdentity', 'SleepStage', 'DerivedTime', 'Power_Band1', 'Power_Band2', 'Power_Band5', 'Power_Band6', 'Power_Band7', 'Power_Band8', 'TD_key2', 'TD_key3', 'TD_count', 'localTime', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ['TD_key2', 'TD_key3']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Running Sequential Feature Selector...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Searching over powerband combos...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Executing Device:  16L\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m [235, 235]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m {'samplingRate': [500], 'gain_1': [235], 'gain_2': [236], 'gain_3': [235], 'gain_4': [235], 'fft_bandFormationConfig': [3], 'fft_interval': [1000], 'fft_size': [1024], 'fft_windowLoad': ['100% Hann'], 'fft_numBins': [512], 'fft_binWidth': [0.48828125], 'Power_Band1': ['0.73-4.15'], 'Power_Band2': ['4.15-12.45'], 'Power_Band3': ['132.08-134.03'], 'Power_Band4': ['128.17-129.64'], 'Power_Band5': ['0.73-4.15'], 'Power_Band6': ['4.64-12.45'], 'Power_Band7': ['17.82-30.03'], 'Power_Band8': ['39.79-63.23'], 'Power_Band1_indices': ['3  9'], 'Power_Band2_indices': ['10  26'], 'Power_Band3_indices': ['272  275'], 'Power_Band4_indices': ['264  266'], 'Power_Band5_indices': ['3  9'], 'Power_Band6_indices': ['11  26'], 'Power_Band7_indices': ['38  62'], 'Power_Band8_indices': ['83  130'], 'Power_Band1_bins': ['0.98-3.91'], 'Power_Band2_bins': ['4.39-12.21'], 'Power_Band3_bins': ['132.32-133.79'], 'Power_Band4_bins': ['128.42-129.39'], 'Power_Band5_bins': ['0.98-3.91'], 'Power_Band6_bins': ['4.88-12.21'], 'Power_Band7_bins': ['18.07-29.79'], 'Power_Band8_bins': ['40.04-62.99']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ['SessionIdentity', 'SleepStage', 'DerivedTime', 'Power_Band1', 'Power_Band2', 'Power_Band5', 'Power_Band6', 'Power_Band7', 'Power_Band8', 'TD_key2', 'TD_key3', 'TD_count', 'localTime', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ['TD_key2', 'TD_key3']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Running Sequential Feature Selector...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Searching over powerband combos...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Executing Device:  03R\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m [231, 234]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m {'samplingRate': [500], 'gain_1': [232], 'gain_2': [231], 'gain_3': [231], 'gain_4': [234], 'fft_bandFormationConfig': [3], 'fft_interval': [1000], 'fft_size': [1024], 'fft_windowLoad': ['100% Hann'], 'fft_numBins': [512], 'fft_binWidth': [0.48828125], 'Power_Band1': ['0.73-4.15'], 'Power_Band2': ['4.64-12.45'], 'Power_Band3': ['132.08-134.03'], 'Power_Band4': ['128.17-129.64'], 'Power_Band5': ['0.73-4.15'], 'Power_Band6': ['4.64-12.45'], 'Power_Band7': ['17.82-30.03'], 'Power_Band8': ['39.79-63.23'], 'Power_Band1_indices': ['3  9'], 'Power_Band2_indices': ['11  26'], 'Power_Band3_indices': ['272  275'], 'Power_Band4_indices': ['264  266'], 'Power_Band5_indices': ['3  9'], 'Power_Band6_indices': ['11  26'], 'Power_Band7_indices': ['38  62'], 'Power_Band8_indices': ['83  130'], 'Power_Band1_bins': ['0.98-3.91'], 'Power_Band2_bins': ['4.88-12.21'], 'Power_Band3_bins': ['132.32-133.79'], 'Power_Band4_bins': ['128.42-129.39'], 'Power_Band5_bins': ['0.98-3.91'], 'Power_Band6_bins': ['4.88-12.21'], 'Power_Band7_bins': ['18.07-29.79'], 'Power_Band8_bins': ['40.04-62.99']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ['SessionIdentity', 'SleepStage', 'DerivedTime', 'Power_Band1', 'Power_Band2', 'Power_Band5', 'Power_Band6', 'Power_Band7', 'Power_Band8', 'TD_key2', 'TD_key3', 'TD_count', 'localTime', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ['TD_key2', 'TD_key3']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Running Sequential Feature Selector...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Searching over powerband combos...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Executing Device:  16R\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m [232, 231]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m {'samplingRate': [500], 'gain_1': [234], 'gain_2': [233], 'gain_3': [232], 'gain_4': [231], 'fft_bandFormationConfig': [3], 'fft_interval': [1000], 'fft_size': [1024], 'fft_windowLoad': ['100% Hann'], 'fft_numBins': [512], 'fft_binWidth': [0.48828125], 'Power_Band1': ['0.73-4.15'], 'Power_Band2': ['4.64-12.45'], 'Power_Band3': ['132.08-134.03'], 'Power_Band4': ['128.17-129.64'], 'Power_Band5': ['0.73-4.15'], 'Power_Band6': ['4.64-12.45'], 'Power_Band7': ['17.82-30.03'], 'Power_Band8': ['39.79-63.23'], 'Power_Band1_indices': ['3  9'], 'Power_Band2_indices': ['11  26'], 'Power_Band3_indices': ['272  275'], 'Power_Band4_indices': ['264  266'], 'Power_Band5_indices': ['3  9'], 'Power_Band6_indices': ['11  26'], 'Power_Band7_indices': ['38  62'], 'Power_Band8_indices': ['83  130'], 'Power_Band1_bins': ['0.98-3.91'], 'Power_Band2_bins': ['4.88-12.21'], 'Power_Band3_bins': ['132.32-133.79'], 'Power_Band4_bins': ['128.42-129.39'], 'Power_Band5_bins': ['0.98-3.91'], 'Power_Band6_bins': ['4.88-12.21'], 'Power_Band7_bins': ['18.07-29.79'], 'Power_Band8_bins': ['40.04-62.99']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ['SessionIdentity', 'SleepStage', 'DerivedTime', 'Power_Band1', 'Power_Band2', 'Power_Band5', 'Power_Band6', 'Power_Band7', 'Power_Band8', 'TD_key2', 'TD_key3', 'TD_count', 'localTime', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m ['TD_key2', 'TD_key3']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Running Sequential Feature Selector...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Searching over powerband combos...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Executing Device:  07L\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Analyzing: TD_key2, TD_key3\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m WARNING: Session numbers provided have different settings.\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Settings dataframe:\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m    SessionIdentity  samplingRate  ...  Power_Band7_bins  Power_Band8_bins\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m 0     07L_05-12-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m 1     07L_05-13-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m 2     07L_05-20-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m 3     07L_05-24-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m 4     07L_05-25-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m 5     07L_06-07-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m 6     07L_06-07-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m 7     07L_06-09-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m 8     07L_06-30-22           500  ...       18.07-29.79       40.04-62.99\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m 9     07L_07-22-22           500  ...       72.27-99.61        5.86-11.72\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m 10    07L_07-26-22           500  ...       72.27-99.61        5.86-11.72\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m [11 rows x 37 columns]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m WARNING: MULTIPLE TIME DOMAIN SETTINGS FOUND for 07L sessions ['07L_06-09-22', '07L_07-22-22', '07L_06-30-22', '07L_05-12-22', '07L_07-26-22', '07L_05-13-22', '07L_05-24-22', '07L_06-07-22', '07L_05-25-22', '07L_05-20-22']. Using the first unique values.\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m [233, 231]\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m {'samplingRate': [500, 500], 'gain_1': [233, 233], 'gain_2': [232, 232], 'gain_3': [233, 233], 'gain_4': [231, 231], 'fft_bandFormationConfig': [3, 3], 'fft_interval': [1000, 50], 'fft_size': [1024, 256], 'fft_windowLoad': ['100% Hann', '100% Hann'], 'fft_numBins': [512, 128], 'fft_binWidth': [0.48828125, 1.953125], 'Power_Band1': ['0.73-4.15', '61.52-71.29'], 'Power_Band2': ['4.15-12.45', '12.70-30.27'], 'Power_Band3': ['132.08-134.03', '4.88-10.74'], 'Power_Band4': ['128.17-129.64', '32.23-59.57'], 'Power_Band5': ['0.73-4.15', '12.70-26.37'], 'Power_Band6': ['4.64-12.45', '71.29-100.59'], 'Power_Band7': ['17.82-30.03', '71.29-100.59'], 'Power_Band8': ['39.79-63.23', '4.88-12.70'], 'Power_Band1_indices': ['3  9', '33  37'], 'Power_Band2_indices': ['10  26', '8  16'], 'Power_Band3_indices': ['272  275', '4  6'], 'Power_Band4_indices': ['264  266', '18  31'], 'Power_Band5_indices': ['3  9', '8  14'], 'Power_Band6_indices': ['11  26', '38  52'], 'Power_Band7_indices': ['38  62', '38  52'], 'Power_Band8_indices': ['83  130', '4  7'], 'Power_Band1_bins': ['0.98-3.91', '62.50-70.31'], 'Power_Band2_bins': ['4.39-12.21', '13.67-29.30'], 'Power_Band3_bins': ['132.32-133.79', '5.86-9.77'], 'Power_Band4_bins': ['128.42-129.39', '33.20-58.59'], 'Power_Band5_bins': ['0.98-3.91', '13.67-25.39'], 'Power_Band6_bins': ['4.88-12.21', '72.27-99.61'], 'Power_Band7_bins': ['18.07-29.79', '72.27-99.61'], 'Power_Band8_bins': ['40.04-62.99', '5.86-11.72']}\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ASSUMING SUBCORTICAL CHANNEL IS CHANNEL 0\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ['SessionIdentity', 'SleepStage', 'DerivedTime', 'Power_Band1', 'Power_Band2', 'Power_Band5', 'Power_Band6', 'Power_Band7', 'Power_Band8', 'TD_key2', 'TD_key3', 'TD_count', 'localTime', 'TD_null']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Simulating FFTs for TD chunks of size 1000 samples\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m ['TD_key2', 'TD_key3']\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Running Sequential Feature Selector...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361675)\u001b[0m Searching over powerband combos...\n",
      "\u001b[2m\u001b[36m(hyperparameter_search_wrapper pid=1361643)\u001b[0m Searching over powerband combos...\n"
     ]
    }
   ],
   "source": [
    "parameters = {'corr_threshold': 0.05, 'num_features_to_select': 35, 'sfs_scoring': 'roc_auc', 'fft_subset_inds':[2,120], 'max_clusters':8, 'TD_Columns': ['TD_key2', 'TD_key3']}\n",
    "sleep_stage_mapping = {2: 1, 3: 1, 4: 0, 5: 0, 6: 0}\n",
    "\n",
    "batches = [['02L', '02R', '03L', '03R', '07L'], ['07R', '09L', '09R', '16L', '16R']]\n",
    "#batches = [['03L', '07R'], ['16L', '16R']]\n",
    "full_batch = [hyperparameter_search_wrapper.remote(batch, parameters, sleep_stage_mapping) for batch in batches]\n",
    "execute = ray.get(full_batch)\n",
    "\n",
    "# TODO: Write a function that takes in list of indicies and returns powerband edges in Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02L\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "02R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715bab9a5ab54fa4a9e6e67302b28b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "03L\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17dbac545894c27990dc7f659ba913c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "03R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd685f38d75f45de96b2ef70893c667f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "07L\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "07R\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "09L\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "09R\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "16L\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "16R\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "yo\n",
      "yup\n",
      "   update_rate  test_accuracy  test_roc_auc  test_balanced_accuracy  \\\n",
      "0            1       0.773409      0.855328                0.751475   \n",
      "1            5       0.839304      0.910213                0.831211   \n",
      "2           10       0.859093      0.928028                0.853971   \n",
      "3           15       0.866331      0.935618                0.862471   \n",
      "4           30       0.877015      0.943806                0.873753   \n",
      "\n",
      "   test_recall  test_precision device  \n",
      "0     0.892940        0.757882    02L  \n",
      "1     0.883538        0.845184    02L  \n",
      "2     0.887112        0.871150    02L  \n",
      "3     0.887338        0.882018    02L  \n",
      "4     0.894749        0.892622    02L  \n",
      "CPU times: user 14min 20s, sys: 23min 50s, total: 38min 10s\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# NOTE: O7 gets bad results because Powerband values were switched in some sessions... need to exclude those sessions for realistic classification results\n",
    "update_rates = [1, 5, 10, 15, 30]\n",
    "devices = ['02L', '02R', '03L', '03R', '07L', '07R', '09L', '09R', '16L', '16R']\n",
    "#devices = ['02L', '02R']\n",
    "sleep_stage_mapping = {2: 1, 3: 1, 4: 0, 5: 0, 6: 0}\n",
    "repeat_cv = False # For just calculating accuracy\n",
    "np.random.seed(seed=1)\n",
    "\n",
    "for i, device in enumerate(devices):\n",
    "   print(device)\n",
    "   if repeat_cv:\n",
    "      scores = []\n",
    "      scores_var = []\n",
    "   else:\n",
    "      #cv_results = {'test_tn': [], 'test_fp': [], 'test_fn': [], 'test_tp': []}\n",
    "      #cv_results = {'test_accuracy': [], 'test_auc': [], 'test_balanced_accuracy': []}\n",
    "      cv_results = {'test_accuracy': [], 'test_roc_auc': [], 'test_balanced_accuracy': [], 'test_recall': [], 'test_precision': []}\n",
    "   df = get_device_as_pl_df(device, con)\n",
    "\n",
    "   for update_rate in update_rates:\n",
    "      data_for_baseline = (df.filter(pl.col('Power_Band8').is_not_null())\n",
    "                           .with_row_count().with_columns([\n",
    "                              pl.col('row_nr') // update_rate\n",
    "                           ])\n",
    "                           .groupby(['row_nr']).agg([\n",
    "                              pl.col('Power_Band5').mean().alias('Power_Band5'),\n",
    "                              pl.col('Power_Band6').mean().alias('Power_Band6'),\n",
    "                              pl.col('Power_Band7').mean().alias('Power_Band7'),\n",
    "                              pl.col('Power_Band8').mean().alias('Power_Band8'),\n",
    "                              pl.col('SleepStage').last().alias('SleepStage')\n",
    "                           ])\n",
    "                           )\n",
    "\n",
    "\n",
    "      # X = PB features\n",
    "      X = data_for_baseline.select(pl.col('^Power_.*$')).to_numpy()\n",
    "      # y = sleep stage labels\n",
    "      y = data_for_baseline.select(\n",
    "         pl.col('SleepStage').map_dict(sleep_stage_mapping)\n",
    "      ).to_numpy().squeeze()\n",
    "      # solver = {‘svd’, ‘lsqr’, ‘eigen’}\n",
    "      model = LinearDiscriminantAnalysis(solver='svd')\n",
    "\n",
    "      if repeat_cv:\n",
    "         # define model evaluation method\n",
    "         cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "         # evaluate model\n",
    "         scores_tmp = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=20)\n",
    "         scores.append(np.mean(scores_tmp))\n",
    "         scores_var.append(np.std(scores_tmp))\n",
    "      else:\n",
    "\n",
    "         cv_results_tmp = cross_validate(model, X, y, cv=5,\n",
    "                      scoring=['accuracy', 'roc_auc', 'balanced_accuracy', 'recall', 'precision'])\n",
    "\n",
    "         [cv_results[k].extend([np.mean(v)]) for (k, v) in cv_results_tmp.items() if k in cv_results.keys()]\n",
    "         \n",
    "   if repeat_cv:\n",
    "      if i == 0:\n",
    "         df_scores = pd.DataFrame({'update_rate': update_rates, 'score': scores, 'score_std': scores_var, 'device': device})\n",
    "      else:\n",
    "         df_tmp = pd.DataFrame({'update_rate': update_rates, 'score': scores, 'score_std': scores_var, 'device': device})\n",
    "         df_scores = pd.concat([df_scores, df_tmp])\n",
    "   else:\n",
    "      if i == 0:\n",
    "         df_scores = pd.DataFrame({'update_rate': update_rates, **cv_results, 'device': device})\n",
    "      else:\n",
    "         df_tmp = pd.DataFrame({'update_rate': update_rates, **cv_results, 'device': device})\n",
    "         df_scores = pd.concat([df_scores, df_tmp])\n",
    "\n",
    "print(df_scores.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom\n",
      "{'fit_time': array([0.00213289, 0.00131989, 0.00129437, 0.0012908 , 0.00128198]), 'score_time': array([0.00290585, 0.00214028, 0.00209594, 0.00210071, 0.00208092]), 'test_accuracy': array([0.52 , 0.52 , 0.52 , 0.5  , 0.515]), 'test_roc_auc': array([0.49659455, 0.52483974, 0.4931891 , 0.48003203, 0.48343509]), 'test_balanced_accuracy': array([0.5       , 0.5       , 0.5       , 0.48813933, 0.50120108])}\n",
      "list\n",
      "{'fit_time': array([0.00129676, 0.00128746, 0.0012908 , 0.00127912, 0.00127935]), 'score_time': array([0.00229549, 0.00209332, 0.00206327, 0.00211072, 0.00208974]), 'test_accuracy': array([0.52 , 0.52 , 0.52 , 0.5  , 0.515]), 'test_roc_auc': array([0.49659455, 0.52483974, 0.4931891 , 0.48003203, 0.48343509]), 'test_balanced_accuracy': array([0.5       , 0.5       , 0.5       , 0.48813933, 0.50120108])}\n"
     ]
    }
   ],
   "source": [
    "def custom_score_dict(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    auc = roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
    "    bal_acc = balanced_accuracy_score(y, y_pred)\n",
    "    return {'accuracy': acc, 'roc_auc': auc, 'balanced_accuracy': bal_acc}\n",
    "\n",
    "X = np.random.rand(1000, 2)\n",
    "y = np.random.randint(0, 2, 1000)\n",
    "model = LinearDiscriminantAnalysis(solver='svd')\n",
    "cv_results = cross_validate(model, X, y, cv=5,\n",
    "                      scoring=custom_score_dict)\n",
    "print('custom')\n",
    "print(cv_results)\n",
    "\n",
    "cv_results_comparison = cross_validate(model, X, y, cv=5,\n",
    "            scoring=['accuracy', 'roc_auc', 'balanced_accuracy'])\n",
    "print('list')\n",
    "print(cv_results_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2909163851bf47a9ada90d0b58ee99c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidgetResampler({\n",
       "    'data': [{'error_y': {'array': array([0.00166065, 0.00384584, 0.00513859, 0.006003…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "px.line(df_scores, x='update_rate', y='score', error_y='score_std', color='device', labels={'x': 'Update rate', 'y': 'Accuracy'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76739193881242ec8e8d1ce1916f363f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidgetResampler({\n",
       "    'data': [{'hovertemplate': 'device=02L<br>update_rate=%{x}<br>test_tp=%{y}<extra><…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(df_scores, x='update_rate', y='test_tp', color='device', labels={'x': 'Update rate', 'y': 'True Positive Rate'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89b8b07ce464d6f8a30706640a479a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidgetResampler({\n",
       "    'data': [{'hovertemplate': 'device=02L<br>update_rate=%{x}<br>test_tn=%{y}<extra><…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(df_scores, x='update_rate', y='test_tn', color='device', labels={'x': 'Update rate', 'y': 'True Negative Rate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>update_rate</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>test_balanced_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.877015</td>\n",
       "      <td>0.943806</td>\n",
       "      <td>0.873753</td>\n",
       "      <td>0.894749</td>\n",
       "      <td>0.892622</td>\n",
       "      <td>02L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.869398</td>\n",
       "      <td>0.935008</td>\n",
       "      <td>0.868966</td>\n",
       "      <td>0.871146</td>\n",
       "      <td>0.907149</td>\n",
       "      <td>02R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.866339</td>\n",
       "      <td>0.935992</td>\n",
       "      <td>0.867729</td>\n",
       "      <td>0.847119</td>\n",
       "      <td>0.896761</td>\n",
       "      <td>03L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.854151</td>\n",
       "      <td>0.934989</td>\n",
       "      <td>0.856023</td>\n",
       "      <td>0.794945</td>\n",
       "      <td>0.910582</td>\n",
       "      <td>03R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.668392</td>\n",
       "      <td>0.760244</td>\n",
       "      <td>0.620820</td>\n",
       "      <td>0.901478</td>\n",
       "      <td>0.658004</td>\n",
       "      <td>07L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.703113</td>\n",
       "      <td>0.728040</td>\n",
       "      <td>0.578734</td>\n",
       "      <td>0.980863</td>\n",
       "      <td>0.693077</td>\n",
       "      <td>07R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.854953</td>\n",
       "      <td>0.915836</td>\n",
       "      <td>0.851244</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.882966</td>\n",
       "      <td>09L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.796285</td>\n",
       "      <td>0.844178</td>\n",
       "      <td>0.788826</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.783636</td>\n",
       "      <td>09R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.789860</td>\n",
       "      <td>0.838137</td>\n",
       "      <td>0.785051</td>\n",
       "      <td>0.815782</td>\n",
       "      <td>0.819893</td>\n",
       "      <td>16L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.833746</td>\n",
       "      <td>0.869149</td>\n",
       "      <td>0.820183</td>\n",
       "      <td>0.900363</td>\n",
       "      <td>0.829904</td>\n",
       "      <td>16R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   update_rate  test_accuracy  test_roc_auc  test_balanced_accuracy  \\\n",
       "4           30       0.877015      0.943806                0.873753   \n",
       "4           30       0.869398      0.935008                0.868966   \n",
       "4           30       0.866339      0.935992                0.867729   \n",
       "4           30       0.854151      0.934989                0.856023   \n",
       "4           30       0.668392      0.760244                0.620820   \n",
       "4           30       0.703113      0.728040                0.578734   \n",
       "4           30       0.854953      0.915836                0.851244   \n",
       "4           30       0.796285      0.844178                0.788826   \n",
       "4           30       0.789860      0.838137                0.785051   \n",
       "4           30       0.833746      0.869149                0.820183   \n",
       "\n",
       "   test_recall  test_precision device  \n",
       "4     0.894749        0.892622    02L  \n",
       "4     0.871146        0.907149    02R  \n",
       "4     0.847119        0.896761    03L  \n",
       "4     0.794945        0.910582    03R  \n",
       "4     0.901478        0.658004    07L  \n",
       "4     0.980863        0.693077    07R  \n",
       "4     0.871200        0.882966    09L  \n",
       "4     0.867500        0.783636    09R  \n",
       "4     0.815782        0.819893    16L  \n",
       "4     0.900363        0.829904    16R  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores[df_scores['update_rate'] == 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a funciton that visualizes the LDA boundary line. Check sleep class directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rcssim_wrapper_parallelized(i, key):\n",
    "    fft, _ = rcssim_wrapper(td_np[i,key], td_np[i,0], sim_settings, gains[key-1])\n",
    "    return fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not speed things up, actually slows it down. Most processes are idle.\n",
    "fft_arr_BG_tmp = Parallel(n_jobs=20)(delayed(rcssim_wrapper_parallelized)(i, 1) for i in range(td_np.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "pool = Pool(processes=20)\n",
    "con = None # Not sure why, but pool.map tries to pickle the duckdb connection object, which fails\n",
    "fft_arr_BG_tmp = pool.map(rcssim_wrapper_parallelized, [(i, 1) for i in range(td_np.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from math import sqrt\n",
    "pool = Pool(processes=20)\n",
    "results = pool.map(sqrt, range(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000000):\n",
    "    sqrt(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4086cd56b3ab5014873d490bb81074c1412e72234c55997d98b6bf97f342707"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
